{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "97f528539d12df3e64d5be5e0e04ebce6084d4e9"
   },
   "source": [
    "# [Predicting House Prices](https://www.kaggle.com/burhanykiyakoglu/predicting-house-prices)\n",
    "[**Burhan Y. Kiyakoglu**](https://www.kaggle.com/burhanykiyakoglu)<BR>\n",
    "[Data Source](https://www.kaggle.com/harlfoxem/housesalesprediction)\n",
    "1. [Overview](#1)\n",
    "1. [Importing Modules, Reading the Dataset and Defining an Evaluation Table](#2)\n",
    "1. [Defining a Function to Calculate the Adjusted $R^{2}$ ](#3)\n",
    "1. [Creating a Simple Linear Regression](#4)\n",
    "    * [Let's Show the Result](#5)\n",
    "1. [Visualizing and Examining Data](#6)\n",
    "    * [Checking Out the Correlation Among Explanatory Variables](#7)\n",
    "1. [Data Preprocessing](#8)\n",
    "    * [Binning](#9)\n",
    "1. [Multiple Regression](#10)    \n",
    "    * [Multiple Regression - 1](#11)\n",
    "    * [Multiple Regression - 2](#12)\n",
    "    * [Multiple Regression - 3](#13)\n",
    "    * [Multiple Regression - 4](#14)\n",
    "1. [Regularization](#15)\n",
    "    * [Ridge Regression](#16)\n",
    "    * [Lasso Regression](#17)\n",
    "1. [Polynomial Regression](#18)  \n",
    "1. [k-NN Regression](#19)\n",
    "1. [Evaluation Table](#20)    \n",
    "1. [Conclusion](#21)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2618a56cc8d9d7b5d2b7fbcca6e33c09d107a846"
   },
   "source": [
    "# <span id=\"1\"></span> Overview\n",
    "<hr/>\n",
    "Welcome to my Kernel! In this kernel, I use various regression methods and try to predict the house prices by using them. As you can guess, there are various methods to suceed this and each method has pros and cons. I think **regression is one of the most important methods because it gives us more insight about the data**. When we ask **why**, it is easier to interpret the relation between the response and explanatory variables. \n",
    "\n",
    "I start with a very simple model and continue with more complex ones after visualizing some features and a data mining process. I try to find the best regression for this dataset. \n",
    "\n",
    "On the other hand, if you are looking for more theory and do not want to use built in functions, I recommend you to check my other kernel [k-NN, Logistic Regression, k-Fold CV from Scratch](https://www.kaggle.com/burhanykiyakoglu/k-nn-logistic-regression-k-fold-cv-from-scratch).\n",
    "\n",
    "If you have a question or feedback, do not hesitate to write and if you like this kernel, please <b><font color=\"red\">do not forget to </font><font color=\"green\">UPVOTE </font></b> ğŸ™‚ \n",
    "\n",
    "<br/>\n",
    "<img src=\"https://i.imgur.com/kCNAFTN.jpg?1\" title=\"source: imgur.com\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "351a6fa94567dc140b2531cf001c6e3eae30cede"
   },
   "source": [
    "# <span id=\"2\"></span> Importing Modules, Reading the Dataset and Defining an Evaluation Table\n",
    "#### [Return Contents](#0)\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3668e1cc3d1f9c85bb910c4512c92d2a52b57aab"
   },
   "source": [
    "In order to make some analysis, we need to set our environment up. To do this, I firstly imported some modules and read data. The below output is the head of the data but if you want to see more details, you might try removing ***#*** signs in front of the ***df.describe()*** and ***df.info()***.  \n",
    "\n",
    "Further, I defined an empty dataframe. This dataframe includes **Root Mean Squared Error (RMSE), R-squared, Adjusted R-squared** and **mean of the R-squared values obtained by the k-Fold Cross Validation**, which are the important metrics to compare different models. Having a **R-squared value closer to one** and **smaller RMSE** means a better fit. In the following sections, I will fill this dataframe with my results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_kg_hide-input": true,
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/kc_house_data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\CTI110~1\\AppData\\Local\\Temp/ipykernel_22992/318883078.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m                            '5-Fold Cross Validation':[]})\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data/kc_house_data.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 586\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    587\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    481\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 482\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    483\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    484\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    810\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 811\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    812\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    813\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1038\u001b[0m             )\n\u001b[0;32m   1039\u001b[0m         \u001b[1;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1040\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1041\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1042\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[1;31m# open handles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[1;34m(self, src, kwds)\u001b[0m\n\u001b[0;32m    220\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m         \"\"\"\n\u001b[1;32m--> 222\u001b[1;33m         self.handles = get_handle(\n\u001b[0m\u001b[0;32m    223\u001b[0m             \u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m             \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    700\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"b\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    701\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 702\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    703\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    704\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/kc_house_data.csv'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import folium\n",
    "from folium.plugins import HeatMap\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "\n",
    "evaluation = pd.DataFrame({'Model': [],\n",
    "                           'Details':[],\n",
    "                           'Root Mean Squared Error (RMSE)':[],\n",
    "                           'R-squared (training)':[],\n",
    "                           'Adjusted R-squared (training)':[],\n",
    "                           'R-squared (test)':[],\n",
    "                           'Adjusted R-squared (test)':[],\n",
    "                           '5-Fold Cross Validation':[]})\n",
    "\n",
    "df = pd.read_csv('data/kc_house_data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f7ddce1d3bc2081cd267d949635e507f1c469f5f"
   },
   "source": [
    "# <span id=\"3\"></span> Defining a Function to Calculate the Adjusted $R^{2}$\n",
    "#### [Return Contents](#0)\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2433b0c95875503e8bc07ff3d537d44502d4f12e"
   },
   "source": [
    "The R-squared increases when the number of features increase. Because of this, sometimes a more robust evaluator is preferred to compare the performance between different models. This evaluater is called adjusted R-squared and it only increases, if the addition of the variable reduces the MSE. The definition of the adjusted $R^{2}$ is:\n",
    "\n",
    "$\\bar{R^{2}}=R^{2}-\\frac{k-1}{n-k}(1-R^{2})$\n",
    "\n",
    "where $n$ is the number of observations and $k$ is the number of parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "çµ±è¨ˆä¸Šçš„é æ¸¬è©•ä¼°è·Ÿæ©Ÿå™¨å­¸ç¿’æœƒæœ‰äº›ä¸åŒã€‚ä¸æœƒç›´æ¥æ¯”è¼ƒé æ¸¬å€¼èˆ‡å¯¦éš›å€¼çš„èª¤å·®ï¼Œè€Œæ˜¯è¨ˆç®—é æ¸¬å€¼èˆ‡å¯¦éš›å€¼ä¹‹é–“çš„ç›¸é—œæ€§ä¿‚æ•¸Rã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2a7590089c0a309d20737a18c84579eca49c62b0"
   },
   "outputs": [],
   "source": [
    "def adjustedR2(r2,n,k):\n",
    "    return r2-(k-1)/(n-k)*(1-r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "74510b5648577407f3fa127c5e37a82525cd3856"
   },
   "source": [
    "# <span id=\"4\"></span> Creating a Simple Linear Regression\n",
    "#### [Return Contents](#0)\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1a40aa787755f3ffa96b0e4bc32586e6419c8007"
   },
   "source": [
    "When we model a linear relationship between a response and **just one** explanatory variable, this is called **simple linear regression**. I want to predict house prices and then, our response variable is price. However, for a simple model we also need to select a feature. When I look at the columns of the dataset, living area (sqft) seemed the most important feature. When we examine the  <a href=#7>correlation matrix</a>, we may observe that price has the highest correlation coefficient with living area (sqft) and this also supports my opinion. Thus, I decided to use **living area (sqft)** as feature but if you want to examine the relationship between price and another feature, you may prefer that feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "1b8d1baa63beba3b46cabc815e566631b6131dd7"
   },
   "outputs": [],
   "source": [
    "#%%capture\n",
    "train_data,test_data = train_test_split(df,train_size = 0.8,random_state=0)\n",
    "\n",
    "lr = linear_model.LinearRegression() #ä½¿ç”¨ç·šæ€§å›æ­¸ä¾†é æ¸¬\n",
    "X_train = np.array(train_data['sqft_living'], dtype=pd.Series).reshape(-1,1) #åªä½¿ç”¨å±…ä½å€ç©ºé–“ç•¶ä½œè‡ªè®Šæ•¸(è¼¸å…¥)\n",
    "y_train = np.array(train_data['price'], dtype=pd.Series) #ä½¿ç”¨åƒ¹æ ¼ç•¶ä½œæ‡‰è®Šæ•¸(è¼¸å‡ºé æ¸¬)\n",
    "lr.fit(X_train,y_train)\n",
    "\n",
    "X_test = np.array(test_data['sqft_living'], dtype=pd.Series).reshape(-1,1) #xåªç”¨å±…ä½ç©ºé–“å»train\n",
    "y_test = np.array(test_data['price'], dtype=pd.Series)\n",
    "\n",
    "pred = lr.predict(X_test)\n",
    "rmsesm = float(format(np.sqrt(metrics.mean_squared_error(y_test,pred)),'.3f')) # Root Mean Squared Error (RMSE)\n",
    "rtrsm = float(format(lr.score(X_train, y_train),'.3f')) # R-squared (training)\n",
    "rtesm = float(format(lr.score(X_test, y_test),'.3f')) # R-squared (test)\n",
    "cv = float(format(cross_val_score(lr,df[['sqft_living']],df['price'],cv=5).mean(),'.3f'))\n",
    "\n",
    "print (\"Average Price for Test Data: {:.3f}\".format(y_test.mean()))\n",
    "print('Intercept: {}'.format(lr.intercept_)) # ç·šæ€§æ–¹ç¨‹å¼ä¸­çš„ base\n",
    "print('Coefficient: {}'.format(lr.coef_)) # ç·šæ€§æ–¹ç¨‹ä¸­çš„ weight\n",
    "\n",
    "r = evaluation.shape[0]\n",
    "evaluation.loc[r] = ['Simple Linear Regression','-',rmsesm,rtrsm,'-',rtesm,'-',cv]\n",
    "evaluation\n",
    "\n",
    "####\n",
    "\n",
    "# R-squared (training) ç›¸é—œæ€§ä¿‚æ•¸ 0.49 -> ä»£è¡¨ç”¨\n",
    "# R-squared (test) æ¸¬è©¦çµ„ç›¸é—œæ€§ä¿‚æ•¸ 0.49 -> è¨“ç·´è³‡æ–™æ²’æœ‰éåº¦ overfit å•é¡Œ\n",
    "# 5-Fold Cross Validation äº¤å‰é©—è­‰"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b9c90a186bd4557928e6180b4e83996a3e1afb81"
   },
   "source": [
    "[](http://)I also printed the intercept and coefficient for the simple linear regression. By using these values and the below definition, we can estimate the house prices manually. The equation we use for our estimations is called hypothesis function and defined as\n",
    "\n",
    "$$h_{\\theta}(X)=\\theta_{0}+\\theta_{1}x$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç”¨ç¬¬ä¸€ç­†æ¸¬è©¦è³‡æ–™ä¾†çœ‹ç·šæ€§å›æ­¸çš„è¨ˆç®— æ²’æœ‰å·®ç•°å¤ªå¤§\n",
    "print('ç›´æ¥ä½¿ç”¨ç·šæ€§æ–¹ç¨‹è¨ˆç®—', X_test[0,0]*lr.coef_ + lr.intercept_)   # x_test * w + v\n",
    "print('åˆ©ç”¨LinearRegressioné æ¸¬', pred[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ce095a1e73c44936202c3c7f6fd2ec334425bfe7"
   },
   "source": [
    "## <span id=\"5\"></span> Let's Show the Result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "bf7a85fa539b7b8ec42316f7e94b67c5e695e31c"
   },
   "source": [
    "Since we have just **two** dimensions at the simple regression, it is **easy to draw** it. The below chart determines the result of the simple regression. It does not look like a perfect fit but when we work with real world datasets, having a perfect fit is not easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "dc26f00656806e4e9bd4955c9cd2e4ba6932c031"
   },
   "outputs": [],
   "source": [
    "sns.set(style=\"white\", font_scale=1) #è¨­å®šSeabornçš„é¡¯ç¤ºé¢¨æ ¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "2947943b51a82e7f0c386679a213f02c5252c4ac"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6.5,5))\n",
    "plt.scatter(X_test,y_test,color='darkgreen',label=\"Data\", alpha=.1)\n",
    "plt.plot(X_test,lr.predict(X_test),color=\"red\",label=\"Predicted Regression Line\")\n",
    "plt.xlabel(\"Living Space (sqft)\", fontsize=15)\n",
    "plt.ylabel(\"Price ($)\", fontsize=15)\n",
    "plt.xticks(fontsize=13)\n",
    "plt.yticks(fontsize=13)\n",
    "plt.legend()\n",
    "\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "plt.gca().spines['top'].set_visible(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6f43bae52db5d8493022153d0a770986d3b49cb9"
   },
   "source": [
    "# <span id=\"6\"></span> Visualizing and Examining Data\n",
    "#### [Return Contents](#0)\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "fe0922167b295a128b237c532e846239cd990207"
   },
   "source": [
    "This is not a very big data and we do not have too many features. Thus, we have chance to plot most of them and reach some useful analytical results. Drawing charts and examining the data before applying a model is a very good practice because we may detect some possible outliers or decide to do normalization. This is not a must but get know the data is always good. Then, I started with the histograms of dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "a8e52b8956be743bff54d7815aafa927013c9ede"
   },
   "outputs": [],
   "source": [
    "df1=df[['price', 'bedrooms', 'bathrooms', 'sqft_living',\n",
    "    'sqft_lot', 'floors', 'waterfront', 'view', 'condition', 'grade',\n",
    "    'sqft_above', 'sqft_basement', 'yr_built', 'yr_renovated', 'zipcode',\n",
    "    'lat', 'long', 'sqft_living15', 'sqft_lot15']]\n",
    "h = df1.hist(bins=25,figsize=(16,16),xlabelsize='10',ylabelsize='10',xrot=-15)\n",
    "sns.despine(left=True, bottom=True)\n",
    "[x.title.set_size(12) for x in h.ravel()];\n",
    "[x.yaxis.tick_left() for x in h.ravel()];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "05515a92ccea03d5a6b3ac25f521c9ff717fe72c"
   },
   "source": [
    "To determine bedrooms, floors or bathrooms/bedrooms vs price, I preferred **boxplot** because we have numerical data but they are not continuous as 1,2,... bedrooms, 2.5, 3,... floors (probably 0.5 stands for the penthouse). \n",
    "\n",
    "From the below charts, it can be seen that there are very few houses which have some features or price appears far from others like 33 bedrooms or price around 7000000. However, determining their possible negative effect will be time consuming and in the real data sets there will always be some outliers like some luxury house prices in this dataset. That's why I am not planning to remove outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "a8e48879cd3346288e5e419a835cf3bd00381d03"
   },
   "outputs": [],
   "source": [
    "sns.set(style=\"whitegrid\", font_scale=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "75fa3cf3dd756a4f05657b3fd62242d4307c1256"
   },
   "outputs": [],
   "source": [
    "f, axes = plt.subplots(1, 2,figsize=(15,5))\n",
    "sns.boxplot(x=df['bedrooms'],y=df['price'], ax=axes[0])\n",
    "sns.boxplot(x=df['floors'],y=df['price'], ax=axes[1])\n",
    "sns.despine(left=True, bottom=True)\n",
    "axes[0].set(xlabel='Bedrooms', ylabel='Price')\n",
    "axes[0].yaxis.tick_left()\n",
    "axes[1].yaxis.set_label_position(\"right\")\n",
    "axes[1].yaxis.tick_right()\n",
    "axes[1].set(xlabel='Floors', ylabel='Price')\n",
    "\n",
    "f, axe = plt.subplots(1, 1,figsize=(12.18,5))\n",
    "sns.despine(left=True, bottom=True)\n",
    "sns.boxplot(x=df['bathrooms'],y=df['price'], ax=axe)\n",
    "axe.yaxis.tick_left()\n",
    "axe.set(xlabel='Bathrooms / Bedrooms', ylabel='Price');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- æˆ¿åƒ¹çš„é›¢ç¾¤å€¼å¾ˆå¤§ ï¼ˆå¾ˆå¤šä½é«˜æ–¼å¹³å‡å€¼ï¼‰\n",
    "- æˆ¿é–“æ•¸ç›®è·Ÿæˆ¿åƒ¹æœ‰æ­£ç›¸é—œæ€§\n",
    "- 2.5å±¤(å¯èƒ½æ˜¯æŒ‡å…©å±¤æ¨“åŠ é–£æ¨“)çš„æˆ¿å­åƒ¹æ ¼æ¯”è¼ƒé«˜\n",
    "- æµ´å®¤/è‡¥å®¤ æ¯”ä¾‹è¶Šå¤§æˆ¿åƒ¹å¹³å‡è¶Šé«˜"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9205c30676daae4db2aac74fca01abc8bb43a957"
   },
   "source": [
    "I drew the price vs some features and it seems that there is not a perfect linear relationship between the price and these features. On the other hand, what about the relationship among each other? To show this, I preferred 3D plots. Also, I used light green as the  point color. Dark green parts mean high density, many light green points overlap and become darker. \n",
    "\n",
    "The below charts show that when the sqrt_living increases, sqrt_lot and bedrooms or bathrooms/bedrooms increases. However, the floors, bedrooms and bathrooms/bedrooms or sqrt_living does not have a similar relationship.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "6e3990e445f4b5d2c5cc0eaff645b331bfcbde53"
   },
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=(19,12.5))\n",
    "ax=fig.add_subplot(2,2,1, projection=\"3d\")\n",
    "ax.scatter(df['floors'],df['bedrooms'],df['bathrooms'],c=\"darkgreen\",alpha=.5)\n",
    "ax.set(xlabel='\\nFloors',ylabel='\\nBedrooms',zlabel='\\nBathrooms / Bedrooms')\n",
    "ax.set(ylim=[0,12])\n",
    "\n",
    "ax=fig.add_subplot(2,2,2, projection=\"3d\")\n",
    "ax.scatter(df['floors'],df['bedrooms'],df['sqft_living'],c=\"darkgreen\",alpha=.5)\n",
    "ax.set(xlabel='\\nFloors',ylabel='\\nBedrooms',zlabel='\\nsqft Living')\n",
    "ax.set(ylim=[0,12])\n",
    "\n",
    "ax=fig.add_subplot(2,2,3, projection=\"3d\")\n",
    "ax.scatter(df['sqft_living'],df['sqft_lot'],df['bathrooms'],c=\"darkgreen\",alpha=.5)\n",
    "ax.set(xlabel='\\n sqft Living',ylabel='\\nsqft Lot',zlabel='\\nBathrooms / Bedrooms')\n",
    "ax.set(ylim=[0,250000])\n",
    "\n",
    "ax=fig.add_subplot(2,2,4, projection=\"3d\")\n",
    "ax.scatter(df['sqft_living'],df['sqft_lot'],df['bedrooms'],c=\"darkgreen\",alpha=.5)\n",
    "ax.set(xlabel='\\n sqft Living',ylabel='\\nsqft Lot',zlabel='Bedrooms')\n",
    "ax.set(ylim=[0,250000]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- å±…ä½ç©ºé–“è¶Šå¤§ï¼Œè‡¥å®¤æ•¸é‡å°±è¶Šå¤šï¼Œç›¸å°çš„ æµ´å®¤/è‡¥å®¤æ¯”ä¾‹ä¹Ÿè¶Šé«˜"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "44a77a6f966b36ef3f0cd4e8784cffd21867b329"
   },
   "source": [
    "Let's visualize more features. When we look at the below boxplots, grade and waterfront effect price visibly. On the other hand, view seem to effect less but it also has an effect on price. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "fcd023eceeb0c92852be68397f2f32bf225bc839"
   },
   "outputs": [],
   "source": [
    "f, axes = plt.subplots(1, 2,figsize=(15,5))\n",
    "sns.boxplot(x=df['waterfront'],y=df['price'], ax=axes[0])\n",
    "sns.boxplot(x=df['view'],y=df['price'], ax=axes[1])\n",
    "sns.despine(left=True, bottom=True)\n",
    "axes[0].set(xlabel='Waterfront', ylabel='Price')\n",
    "axes[0].yaxis.tick_left()\n",
    "axes[1].yaxis.set_label_position(\"right\")\n",
    "axes[1].yaxis.tick_right()\n",
    "axes[1].set(xlabel='View', ylabel='Price')\n",
    "\n",
    "f, axe = plt.subplots(1, 1,figsize=(12.18,5))\n",
    "sns.boxplot(x=df['grade'],y=df['price'], ax=axe)\n",
    "sns.despine(left=True, bottom=True)\n",
    "axe.yaxis.tick_left()\n",
    "axe.set(xlabel='Grade', ylabel='Price');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- æ¿±æ°´å€ã€æœ‰æ™¯è§€ã€ä½å®…ç­‰ç´šéƒ½æ˜¯å½±éŸ¿æˆ¿åƒ¹çš„å› ç´ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2cca18f315d672e732ef55eb6700b2a537a84a7b"
   },
   "source": [
    "Further, I drew the 3D plot to determine the relation between the view, grade and year built. The below chart shows that the newer houses have better grades but we can not say much about the change in the view.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "1f4d2a4718055ee5c4fe83b5e90b0ca4039f883c"
   },
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=(9.5,6.25))\n",
    "ax=fig.add_subplot(1,1,1, projection=\"3d\")\n",
    "ax.scatter(train_data['view'],train_data['grade'],train_data['yr_built'],c=\"darkgreen\",alpha=.5)\n",
    "ax.set(xlabel='\\nView',ylabel='\\nGrade',zlabel='\\nYear Built');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- å»ºé€ å¹´ä»½è¼ƒæ™šçš„æˆ¿å­ï¼Œå±…ä½ç­‰ç´šæœƒæ¯”è¼ƒé«˜"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f084c05894d650f8376e0622689e0a6847d4e336"
   },
   "source": [
    "In this dataset, we have latitude and longtitude information for the houses. By using *lat* and *long* columns, I displayed the below heat map which is very useful for the people who does not know Seattle well. Also, if you select a spesific zip code, you may just see the heat map of this zip code's neighborhood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "d70d0d9269a498a6e3850eb2b0f191b5997f2a6b"
   },
   "outputs": [],
   "source": [
    "# find the row of the house which has the highest price\n",
    "maxpr=df.loc[df['price'].idxmax()]\n",
    "\n",
    "# define a function to draw a basemap easily\n",
    "def generateBaseMap(default_location=[47.5112, -122.257], default_zoom_start=9.4):\n",
    "    base_map = folium.Map(location=default_location, control_scale=True, zoom_start=default_zoom_start)\n",
    "    return base_map\n",
    "\n",
    "df_copy = df.copy()\n",
    "# select a zipcode for the heatmap\n",
    "#set(df['zipcode'])\n",
    "#df_copy = df[df['zipcode']==98001].copy()\n",
    "df_copy['count'] = 1\n",
    "basemap = generateBaseMap()\n",
    "# add carton position map\n",
    "folium.TileLayer('cartodbpositron').add_to(basemap)\n",
    "s=folium.FeatureGroup(name='icon').add_to(basemap)\n",
    "# add a marker for the house which has the highest price\n",
    "folium.Marker([maxpr['lat'], maxpr['long']],popup='Highest Price: $'+str(format(maxpr['price'],'.0f')),\n",
    "              icon=folium.Icon(color='green')).add_to(s)\n",
    "# add heatmap\n",
    "HeatMap(data=df_copy[['lat','long','count']].groupby(['lat','long']).sum().reset_index().values.tolist(),\n",
    "        radius=8,max_zoom=13,name='Heat Map').add_to(basemap)\n",
    "folium.LayerControl(collapsed=False).add_to(basemap)\n",
    "basemap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "8d14da10f3390bcf9ae706accfcf9e8c9fad6a6e"
   },
   "source": [
    "## <span id=\"7\"></span> Checking Out the Correlation Among Explanatory Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "86880e8d92d533d27e6980bbdaff70ab791c69f1"
   },
   "source": [
    "Having too many features in a model is not always a good thing because it might cause overfitting and worser results when we want to predict values for a new dataset. Thus, **if a feature does not improve your model a lot, not adding it may be a better choice**.\n",
    "\n",
    "Another important thing is correlation. **If there is very high correlation between two features, keeping both of them is not a good idea most of the time not to cause overfitting**. For instance, if there is overfitting, we may remove sqt_above or sqt_living because they are highly correlated. This relation can be estimated when we look at the definitions in the dataset but to be sure correlation matrix should be checked. However, this does not mean that you must remove one of the highly correlated features. For example: bathrooms and sqrt_living. They are highly correlated but I do not think that the relation among them is the same as the relation between sqt_living and sqt_above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "9dae422e4240a73b25b811197532d6da8dd68c67"
   },
   "outputs": [],
   "source": [
    "features = ['price','bedrooms','bathrooms','sqft_living','sqft_lot','floors','waterfront',\n",
    "            'view','condition','grade','sqft_above','sqft_basement','yr_built','yr_renovated',\n",
    "            'zipcode','lat','long','sqft_living15','sqft_lot15']\n",
    "\n",
    "mask = np.zeros_like(df[features].corr(), dtype=np.bool) \n",
    "mask[np.triu_indices_from(mask)] = True \n",
    "\n",
    "f, ax = plt.subplots(figsize=(16, 12))\n",
    "plt.title('Pearson Correlation Matrix',fontsize=25)\n",
    "\n",
    "sns.heatmap(df[features].corr(),linewidths=0.25,vmax=0.7,square=True,cmap=\"BuGn\", #\"BuGn_r\" to reverse \n",
    "            linecolor='w',annot=True,annot_kws={\"size\":8},mask=mask,cbar_kws={\"shrink\": .9});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å½±éŸ¿æˆ¿åƒ¹çš„æ¬„ä½æŒ‰ç…§ç›¸é—œæ€§ä¿‚æ•¸ä¾†æ’åˆ—\n",
    "df[features].corr()['price'].reset_index().sort_values('price',ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- æ’åå‰å¹¾é …çš„æ¬„ä½ç›¸äº’ä¹‹é–“çš„é—œè¯æ€§æœ¬ä¾†å°±å¾ˆé«˜\n",
    "- zipcode(å€ç¢¼)é›–ç„¶èˆ‡æˆ¿åƒ¹ç›¸é—œæ€§å¾ˆä½ï¼Œä½†æ˜¯åŸå› æ‡‰è©²åªæ˜¯å› ç‚ºç·¨ç¢¼çš„é—œä¿‚ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7163826c32feb75f8fbf12a0262382defc1ef438"
   },
   "source": [
    "# <span id=\"8\"></span> Data Preprocessing\n",
    "#### [Return Contents](#0)\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "26f84b1ab459f3054acb0881a9717fef59048625"
   },
   "source": [
    "A preprocessing on data might improve the model accuracy and make the model more reliable. It does not always have to improve our results but when we are conscious of the features and use a proper input, we might reach some outcomes easier. I tried various data mining techniques like transformation or normalization but in the end, decided to just use binning and created a new dataframe called ***df_dm***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "0f0141a5ef51c04c4b70549fe114ce2fcf14beeb"
   },
   "outputs": [],
   "source": [
    "df_dm=df.copy()\n",
    "df_dm.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c850d79333d3ce68c2d119397292c2647c8c776e"
   },
   "source": [
    "## <span id=\"9\"></span> Binning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_kg_hide-input": false,
    "_uuid": "4ef7608090f7ea71d4653d85ae9511bbe4f2a364"
   },
   "source": [
    "Data binning is a preprocessing technique used to reduce the effects of minor observation errors. I think it is worthwhile applying to some columns of this dataset. I applied binning to *yr_built* and *yr_renovated*. I added the ages and renovation ages of the houses when they were sold. Also, I partitioned these columns to intervals and you can observe this in the below **histograms**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "47848fce34b0eb77b897eb79da0f0ce0eaa898ef"
   },
   "outputs": [],
   "source": [
    "# just take the year from the date column\n",
    "df_dm['sales_yr']=df_dm['date'].astype(str).str[:4]\n",
    "\n",
    "# add the age of the buildings when the houses were sold as a new column\n",
    "# æ–°å¢æ¬„ä½ å±‹é½¡\n",
    "df_dm['age']=df_dm['sales_yr'].astype(int)-df_dm['yr_built']\n",
    "\n",
    "# add the age of the renovation when the houses were sold as a new column\n",
    "# æ–°å¢æ¬„ä½ è£ä¿®å¹´ä»½\n",
    "df_dm['age_rnv']=0\n",
    "df_dm['age_rnv']=df_dm['sales_yr'][df_dm['yr_renovated']!=0].astype(int)-df_dm['yr_renovated'][df_dm['yr_renovated']!=0]\n",
    "df_dm.loc[df_dm['age_rnv'].isnull(), 'age_rnv'] = 0\n",
    "\n",
    "# partition the age into bins\n",
    "# å°‡å±‹é½¡æŒ‰ç…§å€é–“æ‹†åˆ†\n",
    "bins = [-2,0,5,10,25,50,75,100,100000]\n",
    "labels = ['<1','1-5','6-10','11-25','26-50','51-75','76-100','>100']\n",
    "df_dm['age_binned'] = pd.cut(df_dm['age'], bins=bins, labels=labels)\n",
    "# partition the age_rnv into bins\n",
    "# å°‡è£ä¿®å¹´ä»½æŒ‰ç…§å€é–“æ‹†åˆ†\n",
    "bins = [-2,0,5,10,25,50,75,100000]\n",
    "labels = ['<1','1-5','6-10','11-25','26-50','51-75','>75']\n",
    "df_dm['age_rnv_binned'] = pd.cut(df_dm['age_rnv'], bins=bins, labels=labels)\n",
    "\n",
    "# histograms for the binned columns\n",
    "f, axes = plt.subplots(1, 2,figsize=(15,5))\n",
    "p1=sns.countplot(x=df_dm['age_binned'],ax=axes[0])\n",
    "for p in p1.patches:\n",
    "    height = p.get_height()\n",
    "    p1.text(p.get_x()+p.get_width()/2,height + 50,height,ha=\"center\")   \n",
    "p2=sns.countplot(x=df_dm['age_rnv_binned'],ax=axes[1])\n",
    "sns.despine(left=True, bottom=True)\n",
    "for p in p2.patches:\n",
    "    height = p.get_height()\n",
    "    p2.text(p.get_x()+p.get_width()/2,height + 200,height,ha=\"center\")\n",
    "    \n",
    "axes[0].set(xlabel='Age')\n",
    "axes[0].yaxis.tick_left()\n",
    "axes[1].yaxis.set_label_position(\"right\")\n",
    "axes[1].yaxis.tick_right()\n",
    "axes[1].set(xlabel='Renovation Age');\n",
    "\n",
    "# transform the factor values to be able to use in the model\n",
    "df_dm = pd.get_dummies(df_dm, columns=['age_binned','age_rnv_binned'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_kg_hide-input": false,
    "_uuid": "acc6eeac4dbc720cbbb1559b732e17155b865056"
   },
   "source": [
    "# <span id=\"10\"></span> Multiple Regression\n",
    "#### [Return Contents](#0)\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "967f31a259c6114391aaea6e1960213d8c2ddaf8"
   },
   "source": [
    "I used a <a href=#4>simple linear regression</a> and found a poor fit. In order to improve this model I am planing to add more features. When we have **more than one** feature in a linear regression, it is defined as **multiple regression**. Then, it is time to create some complex models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d3c3012822cf1212799e355729c048d1a12a90a8"
   },
   "source": [
    "## <span id=\"11\"></span> Multiple Regression - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4871f2d6297f1b618f21b565294755947cdccf97"
   },
   "source": [
    "I determined ***features*** at first sight by looking at the previous sections and used in my first multiple linear regression. As in the simple regression, I printed the coefficients which the model uses for the predictions. However, this time we must use the below definition for our predictions, if we want to make calculations manually.\n",
    "\n",
    "$$h_{\\theta}(X)=\\theta_{0}+\\theta_{1}x_{1}+\\theta_{2}x_{2}+...+\\theta_{n}x_{n}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "2cad632d447b8bd2c7acd0420aeaa7ac35759155"
   },
   "outputs": [],
   "source": [
    "# ä½¿ç”¨å…­å€‹æŒ‘é¸å‡ºçš„æ¬„ä½ä¾†è¨“ç·´ç·šæ€§å›æ­¸\n",
    "train_data_dm,test_data_dm = train_test_split(df_dm,train_size = 0.8,random_state=3)\n",
    "\n",
    "features = ['bedrooms','bathrooms','sqft_living','sqft_lot','floors','zipcode']\n",
    "complex_model_1 = linear_model.LinearRegression()\n",
    "complex_model_1.fit(train_data_dm[features],train_data_dm['price'])\n",
    "\n",
    "print('Intercept: {}'.format(complex_model_1.intercept_))\n",
    "print('Coefficients: {}'.format(complex_model_1.coef_))\n",
    "\n",
    "pred = complex_model_1.predict(test_data_dm[features])\n",
    "rmsecm = float(format(np.sqrt(metrics.mean_squared_error(test_data_dm['price'],pred)),'.3f'))\n",
    "rtrcm = float(format(complex_model_1.score(train_data_dm[features],train_data_dm['price']),'.3f'))\n",
    "artrcm = float(format(adjustedR2(complex_model_1.score(train_data_dm[features],train_data_dm['price']),train_data_dm.shape[0],len(features)),'.3f'))\n",
    "rtecm = float(format(complex_model_1.score(test_data_dm[features],test_data_dm['price']),'.3f'))\n",
    "artecm = float(format(adjustedR2(complex_model_1.score(test_data_dm[features],test_data['price']),test_data_dm.shape[0],len(features)),'.3f'))\n",
    "cv = float(format(cross_val_score(complex_model_1,df_dm[features],df_dm['price'],cv=5).mean(),'.3f'))\n",
    "\n",
    "r = evaluation.shape[0]\n",
    "evaluation.loc[r] = ['Multiple Regression-1','selected features',rmsecm,rtrcm,artrcm,rtecm,artecm,cv]\n",
    "evaluation.sort_values(by = '5-Fold Cross Validation', ascending=False)\n",
    "# R-squared (training)\t #0.514\n",
    "# R-squared (test) 0.519 (é‚„æ¯” training é«˜) è·Ÿè¨“ç·´é›†èª¤å·®ä¸å¤§ï¼Œä»£è¡¨æ²’æœ‰overfit \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9fd3947d7e315e659dbf84442c0346efc7b96cca"
   },
   "source": [
    "## <span id=\"12\"></span> Multiple Regression - 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f64e50c07db6143205092a893cef7b825b211634"
   },
   "source": [
    "In addition to the previous subsection, I added more features to the ***features*** list. Also, I printed the coefficients of the model as in the previous subsection. When we look at the evaluation metrics, they improved significantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "d26198b085b6dd109dc844b63ec8ef6e50c787e1"
   },
   "outputs": [],
   "source": [
    "features = ['bedrooms','bathrooms','sqft_living','sqft_lot','floors','waterfront','view',\n",
    "             'grade','age_binned_<1', 'age_binned_1-5', 'age_binned_6-10','age_binned_11-25', \n",
    "             'age_binned_26-50', 'age_binned_51-75','age_binned_76-100', 'age_binned_>100',\n",
    "             'zipcode']\n",
    "complex_model_2 = linear_model.LinearRegression()\n",
    "complex_model_2.fit(train_data_dm[features],train_data_dm['price'])\n",
    "\n",
    "print('Intercept: {}'.format(complex_model_2.intercept_))\n",
    "print('Coefficients: {}'.format(complex_model_2.coef_))\n",
    "\n",
    "pred = complex_model_2.predict(test_data_dm[features])\n",
    "rmsecm = float(format(np.sqrt(metrics.mean_squared_error(test_data_dm['price'],pred)),'.3f'))\n",
    "rtrcm = float(format(complex_model_2.score(train_data_dm[features],train_data_dm['price']),'.3f'))\n",
    "artrcm = float(format(adjustedR2(complex_model_2.score(train_data_dm[features],train_data_dm['price']),train_data_dm.shape[0],len(features)),'.3f'))\n",
    "rtecm = float(format(complex_model_2.score(test_data_dm[features],test_data_dm['price']),'.3f'))\n",
    "artecm = float(format(adjustedR2(complex_model_2.score(test_data_dm[features],test_data_dm['price']),test_data_dm.shape[0],len(features)),'.3f'))\n",
    "cv = float(format(cross_val_score(complex_model_2,df_dm[features],df_dm['price'],cv=5).mean(),'.3f'))\n",
    "\n",
    "r = evaluation.shape[0]\n",
    "evaluation.loc[r] = ['Multiple Regression-2','selected features',rmsecm,rtrcm,artrcm,rtecm,artecm,cv]\n",
    "evaluation.sort_values(by = '5-Fold Cross Validation', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "18ceaa2fe2075790b283df7405efc4c0e299f490"
   },
   "source": [
    "## <span id=\"13\"></span> Multiple Regression - 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e5d10f7cfbb89981b21e891c398d031365a66427"
   },
   "source": [
    "In order to observe differences easily, I created a model with all features without any preprossing. Evaluation metrics improved remarkable again.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "af2c5ad0b23b63761e29b77f9773d1f83b3705ad"
   },
   "outputs": [],
   "source": [
    "features = ['bedrooms','bathrooms','sqft_living','sqft_lot','floors','waterfront','view',\n",
    "            'condition','grade','sqft_above','sqft_basement','yr_built','yr_renovated',\n",
    "            'zipcode','lat','long','sqft_living15','sqft_lot15']\n",
    "complex_model_3 = linear_model.LinearRegression()\n",
    "complex_model_3.fit(train_data[features],train_data['price'])\n",
    "\n",
    "print('Intercept: {}'.format(complex_model_3.intercept_))\n",
    "print('Coefficients: {}'.format(complex_model_3.coef_))\n",
    "\n",
    "pred = complex_model_3.predict(test_data[features])\n",
    "rmsecm = float(format(np.sqrt(metrics.mean_squared_error(test_data['price'],pred)),'.3f'))\n",
    "rtrcm = float(format(complex_model_3.score(train_data[features],train_data['price']),'.3f'))\n",
    "artrcm = float(format(adjustedR2(complex_model_3.score(train_data[features],train_data['price']),train_data.shape[0],len(features)),'.3f'))\n",
    "rtecm = float(format(complex_model_3.score(test_data[features],test_data['price']),'.3f'))\n",
    "artecm = float(format(adjustedR2(complex_model_3.score(test_data[features],test_data['price']),test_data.shape[0],len(features)),'.3f'))\n",
    "cv = float(format(cross_val_score(complex_model_3,df[features],df['price'],cv=5).mean(),'.3f'))\n",
    "\n",
    "r = evaluation.shape[0]\n",
    "evaluation.loc[r] = ['Multiple Regression-3','all features, no preprocessing',rmsecm,rtrcm,artrcm,rtecm,artecm,cv]\n",
    "evaluation.sort_values(by = '5-Fold Cross Validation', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "63e2e4ca2d45380c7371140c31dc2418b3624bd6"
   },
   "source": [
    "## <span id=\"14\"></span> Multiple Regression - 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "800097f19ec6e77ade7b2c3826c4c83d2a30e34b"
   },
   "source": [
    "This time I used the data obtained after preprocessing step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "c8b3db456655efccc35b45cb5970b5f397348eda"
   },
   "outputs": [],
   "source": [
    "features = ['bedrooms','bathrooms','sqft_living','sqft_lot','floors','waterfront',\n",
    "            'view','condition','grade','sqft_above','sqft_basement','age_binned_<1', \n",
    "            'age_binned_1-5', 'age_binned_6-10','age_binned_11-25', 'age_binned_26-50',\n",
    "            'age_binned_51-75','age_binned_76-100', 'age_binned_>100','age_rnv_binned_<1',\n",
    "            'age_rnv_binned_1-5', 'age_rnv_binned_6-10', 'age_rnv_binned_11-25',\n",
    "            'age_rnv_binned_26-50', 'age_rnv_binned_51-75', 'age_rnv_binned_>75',\n",
    "            'zipcode','lat','long','sqft_living15','sqft_lot15']\n",
    "complex_model_4 = linear_model.LinearRegression()\n",
    "complex_model_4.fit(train_data_dm[features],train_data_dm['price'])\n",
    "\n",
    "print('Intercept: {}'.format(complex_model_4.intercept_))\n",
    "print('Coefficients: {}'.format(complex_model_4.coef_))\n",
    "\n",
    "pred = complex_model_4.predict(test_data_dm[features])\n",
    "rmsecm = float(format(np.sqrt(metrics.mean_squared_error(test_data_dm['price'],pred)),'.3f'))\n",
    "rtrcm = float(format(complex_model_4.score(train_data_dm[features],train_data_dm['price']),'.3f'))\n",
    "artrcm = float(format(adjustedR2(complex_model_4.score(train_data_dm[features],train_data_dm['price']),train_data_dm.shape[0],len(features)),'.3f'))\n",
    "rtecm = float(format(complex_model_4.score(test_data_dm[features],test_data_dm['price']),'.3f'))\n",
    "artecm = float(format(adjustedR2(complex_model_4.score(test_data_dm[features],test_data_dm['price']),test_data_dm.shape[0],len(features)),'.3f'))\n",
    "cv = float(format(cross_val_score(complex_model_4,df_dm[features],df_dm['price'],cv=5).mean(),'.3f'))\n",
    "\n",
    "r = evaluation.shape[0]\n",
    "evaluation.loc[r] = ['Multiple Regression-4','all features',rmsecm,rtrcm,artrcm,rtecm,artecm,cv]\n",
    "evaluation.sort_values(by = '5-Fold Cross Validation', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6299701c38af10e514495171e49144ea852d8546"
   },
   "source": [
    "# <span id=\"15\"></span> Regularization\n",
    "#### [Return Contents](#0)\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "166dcc60772db28d1d34a90f53d224a88f042eeb"
   },
   "source": [
    "**Regularization** is designed to address the problem of overfitting and undefitting. **Overfitting** means high variance and is usually caused by a complicated function that creates a lot of unnecessary curves and angles unrelated to the data. This function fits training data well but might cause poor results for the test set. On the other hand, **underfitting** means low variance and a very simple model. This might also cause poor results too. **Possible remedies** are adjusting features manually or using some model selection algoritms which brings an extra workload. Conversely, when we apply regularization, all the features are kept and the model adjusts $\\theta_{j}$. This especially works when we have a lot of slightly useful features. There are two widely used regularization types (Ridge and Lasso Regressions) and in this section, I used them.\n",
    "\n",
    "**When to use ridge vs lasso regression:**\n",
    "*  Many small/medium sized effects: use *ridge*.\n",
    "*  Only a few variables with medium/large effect: use *lasso*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7d196f2de0073f72f02909f43393a8fc29f84138"
   },
   "source": [
    "## <span id=\"16\"></span> Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8388e556b3dc7cf51122d60666d60df487de702a"
   },
   "source": [
    "Ridge regression is called **L2 regularization** and by adding a penalty, we obtain the below equation\n",
    "\n",
    "$$RSS_{RIDGE} = \\sum_{i=1}^{m}(h_{\\theta}(x_{i})-y_{i})^{2} + \\alpha \\sum_{j=1}^{n}\\theta^{2}_{j}$$\n",
    "\n",
    "By changing the $\\alpha$ value, we can control the amount of the regularization. When we increase $\\alpha$, regularization increases and the opposite is valid too. As a result of this, I selected different $\\alpha$ values and used a linear regression without regularization in order to observe the differences easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": false,
    "_uuid": "cbaf99aeea54e18a47f5efa623ba70c68766daed"
   },
   "outputs": [],
   "source": [
    "features = ['bedrooms','bathrooms','sqft_living','sqft_lot','floors','waterfront',\n",
    "            'view','condition','grade','sqft_above','sqft_basement','age_binned_<1', \n",
    "            'age_binned_1-5', 'age_binned_6-10','age_binned_11-25', 'age_binned_26-50',\n",
    "            'age_binned_51-75','age_binned_76-100', 'age_binned_>100','age_rnv_binned_<1',\n",
    "            'age_rnv_binned_1-5', 'age_rnv_binned_6-10', 'age_rnv_binned_11-25',\n",
    "            'age_rnv_binned_26-50', 'age_rnv_binned_51-75', 'age_rnv_binned_>75',\n",
    "            'zipcode','lat','long','sqft_living15','sqft_lot15']\n",
    "complex_model_R = linear_model.Ridge(alpha=1)\n",
    "complex_model_R.fit(train_data_dm[features],train_data_dm['price'])\n",
    "\n",
    "pred1 = complex_model_R.predict(test_data_dm[features])\n",
    "rmsecm1 = float(format(np.sqrt(metrics.mean_squared_error(test_data_dm['price'],pred1)),'.3f'))\n",
    "rtrcm1 = float(format(complex_model_R.score(train_data_dm[features],train_data_dm['price']),'.3f'))\n",
    "artrcm1 = float(format(adjustedR2(complex_model_R.score(train_data_dm[features],train_data_dm['price']),train_data_dm.shape[0],len(features)),'.3f'))\n",
    "rtecm1 = float(format(complex_model_R.score(test_data_dm[features],test_data_dm['price']),'.3f'))\n",
    "artecm1 = float(format(adjustedR2(complex_model_R.score(test_data_dm[features],test_data_dm['price']),test_data_dm.shape[0],len(features)),'.3f'))\n",
    "cv1 = float(format(cross_val_score(complex_model_R,df_dm[features],df_dm['price'],cv=5).mean(),'.3f'))\n",
    "\n",
    "complex_model_R = linear_model.Ridge(alpha=100)\n",
    "complex_model_R.fit(train_data_dm[features],train_data_dm['price'])\n",
    "\n",
    "pred2 = complex_model_R.predict(test_data_dm[features])\n",
    "rmsecm2 = float(format(np.sqrt(metrics.mean_squared_error(test_data_dm['price'],pred2)),'.3f'))\n",
    "rtrcm2 = float(format(complex_model_R.score(train_data_dm[features],train_data_dm['price']),'.3f'))\n",
    "artrcm2 = float(format(adjustedR2(complex_model_R.score(train_data_dm[features],train_data_dm['price']),train_data_dm.shape[0],len(features)),'.3f'))\n",
    "rtecm2 = float(format(complex_model_R.score(test_data_dm[features],test_data_dm['price']),'.3f'))\n",
    "artecm2 = float(format(adjustedR2(complex_model_R.score(test_data_dm[features],test_data_dm['price']),test_data_dm.shape[0],len(features)),'.3f'))\n",
    "cv2 = float(format(cross_val_score(complex_model_R,df_dm[features],df_dm['price'],cv=5).mean(),'.3f'))\n",
    "\n",
    "complex_model_R = linear_model.Ridge(alpha=1000)\n",
    "complex_model_R.fit(train_data_dm[features],train_data_dm['price'])\n",
    "\n",
    "pred3 = complex_model_R.predict(test_data_dm[features])\n",
    "rmsecm3 = float(format(np.sqrt(metrics.mean_squared_error(test_data_dm['price'],pred3)),'.3f'))\n",
    "rtrcm3 = float(format(complex_model_R.score(train_data_dm[features],train_data_dm['price']),'.3f'))\n",
    "artrcm3 = float(format(adjustedR2(complex_model_R.score(train_data_dm[features],train_data_dm['price']),train_data_dm.shape[0],len(features)),'.3f'))\n",
    "rtecm3 = float(format(complex_model_R.score(test_data_dm[features],test_data_dm['price']),'.3f'))\n",
    "artecm3 = float(format(adjustedR2(complex_model_R.score(test_data_dm[features],test_data_dm['price']),test_data_dm.shape[0],len(features)),'.3f'))\n",
    "cv3 = float(format(cross_val_score(complex_model_R,df_dm[features],df_dm['price'],cv=5).mean(),'.3f'))\n",
    "\n",
    "r = evaluation.shape[0]\n",
    "evaluation.loc[r] = ['Ridge Regression','alpha=1, all features',rmsecm1,rtrcm1,artrcm1,rtecm1,artecm1,cv1]\n",
    "evaluation.loc[r+1] = ['Ridge Regression','alpha=100, all features',rmsecm2,rtrcm2,artrcm2,rtecm2,artecm2,cv2]\n",
    "evaluation.loc[r+2] = ['Ridge Regression','alpha=1000, all features',rmsecm3,rtrcm3,artrcm3,rtecm3,artecm3,cv3]\n",
    "evaluation.sort_values(by = '5-Fold Cross Validation', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "10f58a32c2fcf99f00622085807ccc5009bfe3b0"
   },
   "source": [
    "## <span id=\"17\"></span> Lasso Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "282105997be5ed247d3124f62c391dd851ecc914"
   },
   "source": [
    "Lasso regression is called **L1 regularization** and it is defined as\n",
    "\n",
    "$$RSS_{LASSO} = \\sum_{i=1}^{m}(h_{\\theta}(x_{i})-y_{i})^{2} + \\alpha \\sum_{j=1}^{n}|\\theta_{j}|$$\n",
    "\n",
    "The main difference between ridge and lasso is the penalty but $\\alpha$ works the same way.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "604d55a521cc281d7df793493bf7ba731ce0ac7c",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "features = ['bedrooms','bathrooms','sqft_living','sqft_lot','floors','waterfront',\n",
    "            'view','condition','grade','sqft_above','sqft_basement','age_binned_<1', \n",
    "            'age_binned_1-5', 'age_binned_6-10','age_binned_11-25', 'age_binned_26-50',\n",
    "            'age_binned_51-75','age_binned_76-100', 'age_binned_>100','age_rnv_binned_<1',\n",
    "            'age_rnv_binned_1-5', 'age_rnv_binned_6-10', 'age_rnv_binned_11-25',\n",
    "            'age_rnv_binned_26-50', 'age_rnv_binned_51-75', 'age_rnv_binned_>75',\n",
    "            'zipcode','lat','long','sqft_living15','sqft_lot15']\n",
    "complex_model_L = linear_model.Lasso(alpha=1)\n",
    "complex_model_L.fit(train_data_dm[features],train_data_dm['price'])\n",
    "\n",
    "pred1 = complex_model_L.predict(test_data_dm[features])\n",
    "rmsecm1 = float(format(np.sqrt(metrics.mean_squared_error(test_data_dm['price'],pred1)),'.3f'))\n",
    "rtrcm1 = float(format(complex_model_L.score(train_data_dm[features],train_data_dm['price']),'.3f'))\n",
    "artrcm1 = float(format(adjustedR2(complex_model_L.score(train_data_dm[features],train_data_dm['price']),train_data_dm.shape[0],len(features)),'.3f'))\n",
    "rtecm1 = float(format(complex_model_L.score(test_data_dm[features],test_data_dm['price']),'.3f'))\n",
    "artecm1 = float(format(adjustedR2(complex_model_L.score(test_data_dm[features],test_data_dm['price']),test_data_dm.shape[0],len(features)),'.3f'))\n",
    "cv1 = float(format(cross_val_score(complex_model_L,df_dm[features],df_dm['price'],cv=5).mean(),'.3f'))\n",
    "\n",
    "complex_model_L = linear_model.Lasso(alpha=100)\n",
    "complex_model_L.fit(train_data_dm[features],train_data_dm['price'])\n",
    "\n",
    "pred2 = complex_model_L.predict(test_data_dm[features])\n",
    "rmsecm2 = float(format(np.sqrt(metrics.mean_squared_error(test_data_dm['price'],pred2)),'.3f'))\n",
    "rtrcm2 = float(format(complex_model_L.score(train_data_dm[features],train_data_dm['price']),'.3f'))\n",
    "artrcm2 = float(format(adjustedR2(complex_model_L.score(train_data_dm[features],train_data_dm['price']),train_data_dm.shape[0],len(features)),'.3f'))\n",
    "rtecm2 = float(format(complex_model_L.score(test_data_dm[features],test_data_dm['price']),'.3f'))\n",
    "artecm2 = float(format(adjustedR2(complex_model_L.score(test_data_dm[features],test_data_dm['price']),test_data_dm.shape[0],len(features)),'.3f'))\n",
    "cv2 = float(format(cross_val_score(complex_model_L,df_dm[features],df_dm['price'],cv=5).mean(),'.3f'))\n",
    "\n",
    "complex_model_L = linear_model.Lasso(alpha=1000)\n",
    "complex_model_L.fit(train_data_dm[features],train_data_dm['price'])\n",
    "\n",
    "pred3 = complex_model_L.predict(test_data_dm[features])\n",
    "rmsecm3 = float(format(np.sqrt(metrics.mean_squared_error(test_data_dm['price'],pred3)),'.3f'))\n",
    "rtrcm3 = float(format(complex_model_L.score(train_data_dm[features],train_data_dm['price']),'.3f'))\n",
    "artrcm3 = float(format(adjustedR2(complex_model_L.score(train_data_dm[features],train_data_dm['price']),train_data_dm.shape[0],len(features)),'.3f'))\n",
    "rtecm3 = float(format(complex_model_L.score(test_data_dm[features],test_data_dm['price']),'.3f'))\n",
    "artecm3 = float(format(adjustedR2(complex_model_L.score(test_data_dm[features],test_data_dm['price']),test_data_dm.shape[0],len(features)),'.3f'))\n",
    "cv3 = float(format(cross_val_score(complex_model_L,df_dm[features],df_dm['price'],cv=5).mean(),'.3f'))\n",
    "\n",
    "r = evaluation.shape[0]\n",
    "evaluation.loc[r] = ['Lasso Regression','alpha=1, all features',rmsecm1,rtrcm1,artrcm1,rtecm1,artecm1,cv1]\n",
    "evaluation.loc[r+1] = ['Lasso Regression','alpha=100, all features',rmsecm2,rtrcm2,artrcm2,rtecm2,artecm2,cv2]\n",
    "evaluation.loc[r+2] = ['Lasso Regression','alpha=1000, all features',rmsecm3,rtrcm3,artrcm3,rtecm3,artecm3,cv3]\n",
    "evaluation.sort_values(by = '5-Fold Cross Validation', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "497e19b22df6a73661b66c4fbb27f36eaee5dfc5"
   },
   "source": [
    "# <span id=\"18\"></span> Polynomial Regression\n",
    "#### [Return Contents](#0)\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1e12ee292f0aeb249c8126a7b7dc6c2399f29a37"
   },
   "source": [
    "For the linear models, the main idea is to fit a straight line to our data. However, if the data has a quadratic distribution, this time choosing a quadratic function and applying a polynomial transformation may give us better results. This time the hypothesis function is defined as\n",
    "\n",
    "$$h_{\\theta}(X)=\\theta_{0}+\\theta_{1}x+\\theta_{2}x^{2}+...+\\theta_{n}x^{n}$$\n",
    "\n",
    "Since there are many variations for the polynomial regression, I prefered to show results by a new table and it can be seen from the below table that polynomial transformation improved the model fit a lot. On the other hand, while using polynomial transformation and deciding to degree, we should be very careful because it **migh cause overfitting**. Also, in the below table overfitting exists for some models. The 5-fold cross validation metrics are negative or low for these models although they have very high R-squared values for the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "d93b04336bb61c90ca470310d3b3c47f58fec094"
   },
   "outputs": [],
   "source": [
    "evaluation_poly = pd.DataFrame({'Model': [],\n",
    "                                'Details':[],\n",
    "                                'Root Mean Squared Error (RMSE)':[],\n",
    "                                'R-squared (training)':[],\n",
    "                                'Adjusted R-squared (training)':[],\n",
    "                                'R-squared (test)':[],\n",
    "                                'Adjusted R-squared (test)':[],\n",
    "                                '5-Fold Cross Validation':[]})\n",
    "\n",
    "features = ['bedrooms','bathrooms','sqft_living','sqft_lot','floors','waterfront','view',\n",
    "             'grade','yr_built','zipcode']\n",
    "polyfeat = PolynomialFeatures(degree=2)\n",
    "X_allpoly = polyfeat.fit_transform(df[features])\n",
    "X_trainpoly = polyfeat.fit_transform(train_data[features])\n",
    "X_testpoly = polyfeat.fit_transform(test_data[features])\n",
    "poly = linear_model.LinearRegression().fit(X_trainpoly, train_data['price'])\n",
    "\n",
    "pred1 = poly.predict(X_testpoly)\n",
    "rmsepoly1 = float(format(np.sqrt(metrics.mean_squared_error(test_data['price'],pred1)),'.3f'))\n",
    "rtrpoly1 = float(format(poly.score(X_trainpoly,train_data['price']),'.3f'))\n",
    "rtepoly1 = float(format(poly.score(X_testpoly,test_data['price']),'.3f'))\n",
    "cv1 = float(format(cross_val_score(linear_model.LinearRegression(),X_allpoly,df['price'],cv=5).mean(),'.3f'))\n",
    "\n",
    "polyfeat = PolynomialFeatures(degree=3)\n",
    "X_allpoly = polyfeat.fit_transform(df[features])\n",
    "X_trainpoly = polyfeat.fit_transform(train_data[features])\n",
    "X_testpoly = polyfeat.fit_transform(test_data[features])\n",
    "poly = linear_model.LinearRegression().fit(X_trainpoly, train_data['price'])\n",
    "\n",
    "pred2 = poly.predict(X_testpoly)\n",
    "rmsepoly2 = float(format(np.sqrt(metrics.mean_squared_error(test_data['price'],pred2)),'.3f'))\n",
    "rtrpoly2 = float(format(poly.score(X_trainpoly,train_data['price']),'.3f'))\n",
    "rtepoly2 = float(format(poly.score(X_testpoly,test_data['price']),'.3f'))\n",
    "cv2 = float(format(cross_val_score(linear_model.LinearRegression(),X_allpoly,df['price'],cv=5).mean(),'.3f'))\n",
    "\n",
    "features = ['bedrooms','bathrooms','sqft_living','sqft_lot','floors','waterfront','view',\n",
    "            'condition','grade','sqft_above','sqft_basement','yr_built','yr_renovated',\n",
    "            'zipcode','lat','long','sqft_living15','sqft_lot15']\n",
    "polyfeat = PolynomialFeatures(degree=2)\n",
    "X_allpoly = polyfeat.fit_transform(df[features])\n",
    "X_trainpoly = polyfeat.fit_transform(train_data[features])\n",
    "X_testpoly = polyfeat.fit_transform(test_data[features])\n",
    "poly = linear_model.LinearRegression().fit(X_trainpoly, train_data['price'])\n",
    "\n",
    "pred3 = poly.predict(X_testpoly)\n",
    "rmsepoly3 = float(format(np.sqrt(metrics.mean_squared_error(test_data['price'],pred3)),'.3f'))\n",
    "rtrpoly3 = float(format(poly.score(X_trainpoly,train_data['price']),'.3f'))\n",
    "rtepoly3 = float(format(poly.score(X_testpoly,test_data['price']),'.3f'))\n",
    "cv3 = float(format(cross_val_score(linear_model.LinearRegression(),X_allpoly,df['price'],cv=5).mean(),'.3f'))\n",
    "\n",
    "polyfeat = PolynomialFeatures(degree=3)\n",
    "X_allpoly = polyfeat.fit_transform(df[features])\n",
    "X_trainpoly = polyfeat.fit_transform(train_data[features])\n",
    "X_testpoly = polyfeat.fit_transform(test_data[features])\n",
    "poly = linear_model.LinearRegression().fit(X_trainpoly, train_data['price'])\n",
    "\n",
    "pred4 = poly.predict(X_testpoly)\n",
    "rmsepoly4 = float(format(np.sqrt(metrics.mean_squared_error(test_data['price'],pred4)),'.3f'))\n",
    "rtrpoly4 = float(format(poly.score(X_trainpoly,train_data['price']),'.3f'))\n",
    "rtepoly4 = float(format(poly.score(X_testpoly,test_data['price']),'.3f'))\n",
    "cv4 = float(format(cross_val_score(linear_model.LinearRegression(),X_allpoly,df['price'],cv=5).mean(),'.3f'))\n",
    "\n",
    "\n",
    "features = ['bedrooms','bathrooms','sqft_living','sqft_lot','floors','waterfront',\n",
    "            'view','condition','grade','sqft_above','sqft_basement','age_binned_<1', \n",
    "            'age_binned_1-5', 'age_binned_6-10','age_binned_11-25', 'age_binned_26-50',\n",
    "            'age_binned_51-75','age_binned_76-100', 'age_binned_>100','age_rnv_binned_<1',\n",
    "            'age_rnv_binned_1-5', 'age_rnv_binned_6-10', 'age_rnv_binned_11-25',\n",
    "            'age_rnv_binned_26-50', 'age_rnv_binned_51-75', 'age_rnv_binned_>75',\n",
    "            'zipcode','lat','long','sqft_living15','sqft_lot15']\n",
    "polyfeat = PolynomialFeatures(degree=2)\n",
    "X_allpoly = polyfeat.fit_transform(df_dm[features])\n",
    "X_trainpoly = polyfeat.fit_transform(train_data_dm[features])\n",
    "X_testpoly = polyfeat.fit_transform(test_data_dm[features])\n",
    "poly = linear_model.LinearRegression().fit(X_trainpoly, train_data['price'])\n",
    "\n",
    "pred5 = poly.predict(X_testpoly)\n",
    "rmsepoly5 = float(format(np.sqrt(metrics.mean_squared_error(test_data_dm['price'],pred5)),'.3f'))\n",
    "rtrpoly5 = float(format(poly.score(X_trainpoly,train_data_dm['price']),'.3f'))\n",
    "rtepoly5 = float(format(poly.score(X_testpoly,test_data_dm['price']),'.3f'))\n",
    "cv5 = float(format(cross_val_score(linear_model.LinearRegression(),X_allpoly,df_dm['price'],cv=5).mean(),'.3f'))\n",
    "\n",
    "polyfeat = PolynomialFeatures(degree=2)\n",
    "X_allpoly = polyfeat.fit_transform(df_dm[features])\n",
    "X_trainpoly = polyfeat.fit_transform(train_data_dm[features])\n",
    "X_testpoly = polyfeat.fit_transform(test_data_dm[features])\n",
    "poly = linear_model.Ridge(alpha=1).fit(X_trainpoly, train_data['price'])\n",
    "\n",
    "pred6 = poly.predict(X_testpoly)\n",
    "rmsepoly6 = float(format(np.sqrt(metrics.mean_squared_error(test_data_dm['price'],pred6)),'.3f'))\n",
    "rtrpoly6 = float(format(poly.score(X_trainpoly,train_data_dm['price']),'.3f'))\n",
    "rtepoly6 = float(format(poly.score(X_testpoly,test_data_dm['price']),'.3f'))\n",
    "cv6 = float(format(cross_val_score(linear_model.Ridge(alpha=1),X_allpoly,df_dm['price'],cv=5).mean(),'.3f'))\n",
    "\n",
    "polyfeat = PolynomialFeatures(degree=2)\n",
    "X_allpoly = polyfeat.fit_transform(df_dm[features])\n",
    "X_trainpoly = polyfeat.fit_transform(train_data_dm[features])\n",
    "X_testpoly = polyfeat.fit_transform(test_data_dm[features])\n",
    "poly = linear_model.Ridge(alpha=50000).fit(X_trainpoly, train_data['price'])\n",
    "\n",
    "pred7 = poly.predict(X_testpoly)\n",
    "rmsepoly7 = float(format(np.sqrt(metrics.mean_squared_error(test_data_dm['price'],pred7)),'.3f'))\n",
    "rtrpoly7 = float(format(poly.score(X_trainpoly,train_data_dm['price']),'.3f'))\n",
    "rtepoly7 = float(format(poly.score(X_testpoly,test_data_dm['price']),'.3f'))\n",
    "cv7 = float(format(cross_val_score(linear_model.Ridge(alpha=50000),X_allpoly,df_dm['price'],cv=5).mean(),'.3f'))\n",
    "\n",
    "polyfeat = PolynomialFeatures(degree=2)\n",
    "X_allpoly = polyfeat.fit_transform(df_dm[features])\n",
    "X_trainpoly = polyfeat.fit_transform(train_data_dm[features])\n",
    "X_testpoly = polyfeat.fit_transform(test_data_dm[features])\n",
    "poly = linear_model.Lasso(alpha=1).fit(X_trainpoly, train_data['price'])\n",
    "\n",
    "pred8 = poly.predict(X_testpoly)\n",
    "rmsepoly8 = float(format(np.sqrt(metrics.mean_squared_error(test_data_dm['price'],pred8)),'.3f'))\n",
    "rtrpoly8 = float(format(poly.score(X_trainpoly,train_data_dm['price']),'.3f'))\n",
    "rtepoly8 = float(format(poly.score(X_testpoly,test_data_dm['price']),'.3f'))\n",
    "cv8 = float(format(cross_val_score(linear_model.Lasso(alpha=1),X_allpoly,df_dm['price'],cv=5).mean(),'.3f'))\n",
    "\n",
    "polyfeat = PolynomialFeatures(degree=2)\n",
    "X_allpoly = polyfeat.fit_transform(df_dm[features])\n",
    "X_trainpoly = polyfeat.fit_transform(train_data_dm[features])\n",
    "X_testpoly = polyfeat.fit_transform(test_data_dm[features])\n",
    "poly = linear_model.Lasso(alpha=50000).fit(X_trainpoly, train_data['price'])\n",
    "\n",
    "pred9 = poly.predict(X_testpoly)\n",
    "rmsepoly9 = float(format(np.sqrt(metrics.mean_squared_error(test_data_dm['price'],pred9)),'.3f'))\n",
    "rtrpoly9 = float(format(poly.score(X_trainpoly,train_data_dm['price']),'.3f'))\n",
    "rtepoly9 = float(format(poly.score(X_testpoly,test_data_dm['price']),'.3f'))\n",
    "cv9 = float(format(cross_val_score(linear_model.Lasso(alpha=50000),X_allpoly,df_dm['price'],cv=5).mean(),'.3f'))\n",
    "\n",
    "r = evaluation_poly.shape[0]\n",
    "evaluation_poly.loc[r] = ['Polynomial Regression','degree=2, selected features, no preprocessing',rmsepoly1,rtrpoly1,'-',rtepoly1,'-',cv1]\n",
    "evaluation_poly.loc[r+1] = ['Polynomial Regression','degree=3, selected features, no preprocessing',rmsepoly2,rtrpoly2,'-',rtepoly2,'-',cv2]\n",
    "evaluation_poly.loc[r+2] = ['Polynomial Regression','degree=2, all features, no preprocessing',rmsepoly3,rtrpoly3,'-',rtepoly3,'-',cv3]\n",
    "evaluation_poly.loc[r+3] = ['Polynomial Regression','degree=3, all features, no preprocessing',rmsepoly4,rtrpoly4,'-',rtepoly4,'-',cv4]\n",
    "evaluation_poly.loc[r+4] = ['Polynomial Regression','degree=2, all features',rmsepoly5,rtrpoly5,'-',rtepoly5,'-',cv5]\n",
    "evaluation_poly.loc[r+5] = ['Polynomial Ridge Regression','alpha=1, degree=2, all features',rmsepoly6,rtrpoly6,'-',rtepoly6,'-',cv6]\n",
    "evaluation_poly.loc[r+6] = ['Polynomial Ridge Regression','alpha=50000, degree=2, all features',rmsepoly7,rtrpoly7,'-',rtepoly7,'-',cv7]\n",
    "evaluation_poly.loc[r+7] = ['Polynomial Lasso Regression','alpha=1, degree=2, all features',rmsepoly8,rtrpoly8,'-',rtepoly8,'-',cv8]\n",
    "evaluation_poly.loc[r+8] = ['Polynomial Lasso Regression','alpha=50000, degree=2, all features',rmsepoly9,rtrpoly9,'-',rtepoly9,'-',cv9]\n",
    "evaluation_poly_temp = evaluation_poly[['Model','Details','Root Mean Squared Error (RMSE)','R-squared (training)','R-squared (test)','5-Fold Cross Validation']]\n",
    "evaluation_poly_temp.sort_values(by = '5-Fold Cross Validation', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d07b139ea0a455852227fc47099dd03c7e541f2e"
   },
   "source": [
    "# <span id=\"19\"></span> k-NN Regression \n",
    "#### [Return Contents](#0)\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4021ef4bde63bc38c56d4bd070fcbf16ba45234a"
   },
   "source": [
    "I included k-NN regression in this kernel but actually, I did not expect to obtain a good result by using it. Also, k-NN does not give much insight. It is a very simple method and the idea behind the algorithm is similar to the k-NN classification. Briefly, it uses the weighted average, median or another statistic you want of k-nearest instances.\n",
    "\n",
    "The evaluation metrics for the training, test sets and different *k* values were given in the below table. It can be seen that, k-NN was not very successful as expected. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "83a2fe657df3cc80e690c35288e2fcebfcaa8743"
   },
   "outputs": [],
   "source": [
    "features = ['bedrooms','bathrooms','sqft_living','sqft_lot','floors','waterfront',\n",
    "            'view','condition','grade','sqft_above','sqft_basement','age_binned_<1', \n",
    "            'age_binned_1-5', 'age_binned_6-10','age_binned_11-25', 'age_binned_26-50',\n",
    "            'age_binned_51-75','age_binned_76-100', 'age_binned_>100','age_rnv_binned_<1',\n",
    "            'age_rnv_binned_1-5', 'age_rnv_binned_6-10', 'age_rnv_binned_11-25',\n",
    "            'age_rnv_binned_26-50', 'age_rnv_binned_51-75', 'age_rnv_binned_>75',\n",
    "            'zipcode','lat','long','sqft_living15','sqft_lot15']\n",
    "knnreg = KNeighborsRegressor(n_neighbors=15)\n",
    "knnreg.fit(train_data_dm[features],train_data_dm['price'])\n",
    "pred = knnreg.predict(test_data_dm[features])\n",
    "\n",
    "rmseknn1 = float(format(np.sqrt(metrics.mean_squared_error(y_test,pred)),'.3f'))\n",
    "rtrknn1 = float(format(knnreg.score(train_data_dm[features],train_data_dm['price']),'.3f'))\n",
    "artrknn1 = float(format(adjustedR2(knnreg.score(train_data_dm[features],train_data_dm['price']),train_data_dm.shape[0],len(features)),'.3f'))\n",
    "rteknn1 = float(format(knnreg.score(test_data_dm[features],test_data_dm['price']),'.3f'))\n",
    "arteknn1 = float(format(adjustedR2(knnreg.score(test_data_dm[features],test_data_dm['price']),test_data_dm.shape[0],len(features)),'.3f'))\n",
    "cv1 = float(format(cross_val_score(knnreg,df_dm[features],df_dm['price'],cv=5).mean(),'.3f'))\n",
    "\n",
    "knnreg = KNeighborsRegressor(n_neighbors=25)\n",
    "knnreg.fit(train_data_dm[features],train_data_dm['price'])\n",
    "pred = knnreg.predict(test_data_dm[features])\n",
    "\n",
    "rmseknn2 = float(format(np.sqrt(metrics.mean_squared_error(y_test,pred)),'.3f'))\n",
    "rtrknn2 = float(format(knnreg.score(train_data_dm[features],train_data_dm['price']),'.3f'))\n",
    "artrknn2 = float(format(adjustedR2(knnreg.score(train_data_dm[features],train_data_dm['price']),train_data_dm.shape[0],len(features)),'.3f'))\n",
    "rteknn2 = float(format(knnreg.score(test_data_dm[features],test_data_dm['price']),'.3f'))\n",
    "arteknn2 = float(format(adjustedR2(knnreg.score(test_data_dm[features],test_data_dm['price']),test_data_dm.shape[0],len(features)),'.3f'))\n",
    "cv2 = float(format(cross_val_score(knnreg,df_dm[features],df_dm['price'],cv=5).mean(),'.3f'))\n",
    "\n",
    "knnreg = KNeighborsRegressor(n_neighbors=27)\n",
    "knnreg.fit(train_data_dm[features],train_data_dm['price'])\n",
    "pred = knnreg.predict(test_data_dm[features])\n",
    "\n",
    "rmseknn3 = float(format(np.sqrt(metrics.mean_squared_error(y_test,pred)),'.3f'))\n",
    "rtrknn3 = float(format(knnreg.score(train_data_dm[features],train_data_dm['price']),'.3f'))\n",
    "artrknn3 = float(format(adjustedR2(knnreg.score(train_data_dm[features],train_data_dm['price']),train_data_dm.shape[0],len(features)),'.3f'))\n",
    "rteknn3 = float(format(knnreg.score(test_data_dm[features],test_data_dm['price']),'.3f'))\n",
    "arteknn3 = float(format(adjustedR2(knnreg.score(test_data_dm[features],test_data_dm['price']),test_data_dm.shape[0],len(features)),'.3f'))\n",
    "cv3 = float(format(cross_val_score(knnreg,df_dm[features],df_dm['price'],cv=5).mean(),'.3f'))\n",
    "\n",
    "r = evaluation.shape[0]\n",
    "evaluation.loc[r] = ['KNN Regression','k=15, all features',rmseknn1,rtrknn1,artrknn1,rteknn1,arteknn1,cv1]\n",
    "evaluation.loc[r+1] = ['KNN Regression','k=25, all features',rmseknn2,rtrknn2,artrknn2,rteknn2,arteknn2,cv2]\n",
    "evaluation.loc[r+2] = ['KNN Regression','k=27, all features',rmseknn3,rtrknn3,artrknn3,rteknn3,arteknn3,cv3]\n",
    "evaluation.sort_values(by = '5-Fold Cross Validation', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span id=\"20\"></span> Evaluation Table\n",
    "#### [Return Contents](#0)\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "evaluation_temp=evaluation.append(evaluation_poly)\n",
    "evaluation_temp1=evaluation_temp.sort_values(by = '5-Fold Cross Validation', ascending=False)\n",
    "evaluation_temp2=evaluation_temp1.reset_index()\n",
    "evaluation_f=evaluation_temp2.iloc[:,1:]\n",
    "evaluation_f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b02ebdaf3872558184c20f8e5081458b9faea5c6"
   },
   "source": [
    "# <span id=\"21\"></span> Conclusion\n",
    "#### [Return Contents](#0)\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "fb9ff6ecdda37bb94ac271d25a5c2e2cc7084692"
   },
   "source": [
    "When we look at the evaluation table, **2nd degree polynomial (all features, no preprocessing)** is the best. However, I have doubts about its reliability. I would prefer the **polynomial ridge regression (alpha=50000, degree=2, all features)** but other models might be useful depending to the situation too. \n",
    "\n",
    "In this kernel, I used scikit-learn which provides me all built in functions I need but if you want to examine the **theory** behind the **logistic regression**, which is used for classification and similar to the linear regression, and some other methods, you might also want to check my other kernel [k-NN, Logistic Regression, k-Fold CV from Scratch](https://www.kaggle.com/burhanykiyakoglu/k-nn-logistic-regression-k-fold-cv-from-scratch).\n",
    "\n",
    "<b><font color=\"green\">Thank you for reading my kernel </font></b> **and If you liked it, please** <b><font color=\"red\">do not forget to <b></font><font color=\"green\">UPVOTE </font></b> ğŸ™‚ "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
