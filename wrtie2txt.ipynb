{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d23b6c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "path = 'test.txt'\n",
    "f = open(path, 'w')\n",
    "f.write('Hello World')\n",
    "f.write(\"123\")\n",
    "f.write(\"123.45\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ee8ffff",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'test.txt'\n",
    "f = open(path, 'w')\n",
    "lines = ['Hello World\\n', '123', '456\\n', '789\\n']\n",
    "f.writelines(lines)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbcb6afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'test.txt'\n",
    "f = open(path, 'a')\n",
    "lines = ['Hello World\\n', '123', '456\\n', '789\\n']\n",
    "f.writelines(lines)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d70eb4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'test.txt'\n",
    "f = open(path, 'w')\n",
    "print('Hello World', file=f)\n",
    "print('123', file=f)\n",
    "print('456', file=f)\n",
    "print('789', file=f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2ff1dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'test.txt'\n",
    "with open(path, 'w') as f:\n",
    "    f.write('apple\\n')\n",
    "    f.write('banana\\n')\n",
    "    f.write('lemon\\n')\n",
    "    f.write('tomato\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "debc9aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'test.txt'\n",
    "with open(path, 'a') as f:\n",
    "    f.write('apple\\n')\n",
    "    f.write('banana\\n')\n",
    "    f.write('tomato\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74ee1879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "google access ok\n",
      "獲取標題筆數共： 31 筆\n",
      "獲取搜尋量筆數共： 31 筆\n",
      "獲取內文筆數共： 31 筆\n",
      "獲取超連結筆數共： 31 筆\n",
      "text:吳霏\n",
      "text:謝京穎\n",
      "事件： 謝京穎\n",
      "text:曹興誠\n",
      "事件： 曹興誠\n",
      "text:楊丞琳\n",
      "事件： 楊丞琳\n",
      "text:曼城\n",
      "地名： 曼城\n",
      "text:薩爾達\n",
      "事件： 薩爾達\n",
      "text:川口春奈\n",
      "地名： 川口\n",
      "事件： 春奈\n",
      "text:萬客什鍋\n",
      "事件： 萬客什鍋\n",
      "text:POCO F5\n",
      "事件： POCO\n",
      "事件： F5\n",
      "text:TEEN TOP\n",
      "事件： TEEN\n",
      "事件： TOP\n",
      "text:元晶\n",
      "事件： 元晶\n",
      "text:勇士\n",
      "事件： 勇士\n",
      "text:Coldplay\n",
      "事件： Coldplay\n",
      "text:李大維\n",
      "事件： 李大維\n",
      "text:陳敏薰\n",
      "人名： 陳敏薰\n",
      "text:藏壽司旗艦店\n",
      "事件： 壽司\n",
      "事件： 旗艦\n",
      "text:楊家俍\n",
      "事件： 楊家俍\n",
      "text:長月燼明結局\n",
      "事件： 月燼\n",
      "事件： 結局\n",
      "text:魔法阿嬤\n",
      "事件： 魔法\n",
      "事件： 阿嬤\n",
      "text:巨齒鯊2\n",
      "事件： 巨齒\n",
      "text:挖呀挖老師\n",
      "事件： 老師\n",
      "text:陶喆\n",
      "事件： 陶喆\n",
      "text:巨齒鯊\n",
      "事件： 巨齒\n",
      "text:堆高機\n",
      "事件： 堆高機\n",
      "text:熱火\n",
      "事件： 熱火\n",
      "text:鄭容和\n",
      "事件： 鄭容\n",
      "text:CPI\n",
      "事件： CPI\n",
      "text:茲卡病毒\n",
      "地名： 茲卡\n",
      "事件： 病毒\n",
      "text:Sum41\n",
      "事件： Sum\n",
      "事件： 41\n",
      "text:蔡天鳳\n",
      "事件： 蔡天鳳\n",
      "text:光與夜之戀\n",
      "事件： 夜之戀\n",
      "沒有重複的內容\n",
      "有重複的內容： {'美國人'}\n",
      "重複的內容已自動刪除\n",
      "有重複的內容： {'謝京穎', '蔡天鳳', '楊家俍', '曹興誠', '楊丞琳', '李大維', '薩爾達'}\n",
      "重複的內容已自動刪除\n"
     ]
    }
   ],
   "source": [
    "import requests,json,re,os,time\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "from jsonpath import jsonpath\n",
    "from fake_useragent import UserAgent\n",
    "#pip install jieba\n",
    "#pip install BeautifulSoup4\n",
    "#pip install jsonpath\n",
    "#pip install fake-useragent\n",
    "\n",
    "# Google Trend\n",
    "# 教學網址 https://tlyu0419.github.io/2020/02/18/Crawl-GoogleTrends/\n",
    "\n",
    "\n",
    "# 設定 line 推播函式\n",
    "# def lineNotifyMessage(token, msg):\n",
    "#     headers = {\n",
    "#         \"Authorization\": \"Bearer \" + token, \n",
    "#         \"Content-Type\" : \"application/x-www-form-urlencoded\"\n",
    "#     }\n",
    "\n",
    "#     payload = {'message': msg }\n",
    "#     r = requests.post(\"https://notify-api.line.me/api/notify\", headers = headers, params = payload)\n",
    "#     return r.status_code\n",
    "# token = 'm6uafWsyIziRWXaqYfTKJxNShGYlp3WM3RG9e0hP2OA'\n",
    "\n",
    "\n",
    "time_trend = datetime.now().strftime('%Y%m%d')\n",
    "time_trend = str(time_trend)\n",
    "\n",
    "# 嘗試編碼擷取資料，若出現錯誤就中斷執行\n",
    "try:\n",
    "    # 修改網址為目前時間\n",
    "    trends_url = 'https://trends.google.com.tw/trends/api/dailytrends?hl=zh-TW&tz=-480&ed={}&geo=TW&ns=15'.format(time_trend)\n",
    "\n",
    "    res_trends = requests.get(trends_url)\n",
    "\n",
    "    # 把干擾文字 )]}',\\n 刪除 \n",
    "    res_trends = re.sub(r'\\)\\]\\}\\',\\n', '', res_trends.text)\n",
    "\n",
    "    # 轉碼，不然不能看\n",
    "\n",
    "    trends = res_trends.encode('utf-8').decode('unicode_escape')\n",
    "\n",
    "\n",
    "    # 把json檔再轉成字串開始做正則處理\n",
    "    trends_str = str(trends)\n",
    "    trends_str = trends_str.replace('\\\"','\\'')\n",
    "\n",
    "    trends_content = re.findall(\"'query'.*?'snippet':\",trends_str)\n",
    "\n",
    "    trends_title_list = []\n",
    "    trends_search_list = []\n",
    "    trends_articles_list = []\n",
    "    trends_url_list = []\n",
    "    count_trends = 0\n",
    "\n",
    "    for content in trends_content:\n",
    "        count_trends+=1\n",
    "        # 洗出標題\n",
    "        trends_title = re.findall(\"'query'.*?,\",content)\n",
    "        trends_title = trends_title[0]\n",
    "        trends_title = trends_title[9:-2]\n",
    "        trends_title_list.append(trends_title)\n",
    "\n",
    "        # 洗出搜尋量\n",
    "        trends_search = re.findall(\"formattedTraffic.*?,\",content)\n",
    "        trends_search = trends_search[0]\n",
    "        trends_search = trends_search[19:-2]\n",
    "        trends_search_list.append(trends_search)    \n",
    "\n",
    "        # 洗出第一則文章\n",
    "        trends_articles = re.findall(\"'articles.*?timeAgo\",content)\n",
    "        trends_articles = trends_articles[0]\n",
    "        trends_articles = trends_articles[22:-10]\n",
    "        trends_articles_list.append(trends_articles)\n",
    "        \n",
    "        # 洗出文章超連結，抓不到就用google搜尋替代\n",
    "        if 'newsUrl' or '\\'Url' in content:\n",
    "            trends_url = re.findall(\"newsUrl.*?',|'url':.*?',\",content)\n",
    "            trends_url = trends_url[0]\n",
    "            trends_url = re.findall(\"http.*?',\",content)\n",
    "            trends_url = trends_url[0]\n",
    "            trends_url = trends_url[:-2]\n",
    "            trends_url_list.append(trends_url)\n",
    "        else:\n",
    "            trends_url = 'https://www.google.com/search?q='+trends_title\n",
    "            trends_url_list.append(trends_url)\n",
    "\n",
    "\n",
    "    print('google access ok')\n",
    "    print('獲取標題筆數共： {} 筆'.format(len(trends_title_list)))\n",
    "    print('獲取搜尋量筆數共： {} 筆'.format(len(trends_search_list)))\n",
    "    print('獲取內文筆數共： {} 筆'.format(len(trends_articles_list)))\n",
    "    print('獲取超連結筆數共： {} 筆'.format(len(trends_url_list)))\n",
    "\n",
    "\n",
    "    ################ 編寫網頁內容  ################\n",
    "\n",
    "    rw = open('separate/google.html','w',encoding = 'utf8')\n",
    "    # r 讀取\n",
    "    # w 寫入(刪除原本內容)\n",
    "    # a 追加寫入\n",
    "\n",
    "\n",
    "    # 寫入google trends\n",
    "    title_trends = '<h2 style=\"text-align:center;background: #f00; color: #fff; margin: 0 4px; border-radius: 4px;\">Google搜尋趨勢</h2>'\n",
    "    rw.write(title_trends)\n",
    "\n",
    "    # 寫入資料擷取時間\n",
    "    time_now = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    time_news = '<div><h3 style=\"text-align:center;color:gray\">資料擷取時間{}</h2></div>'.format(time_now)\n",
    "    rw.write(time_news)\n",
    "\n",
    "    for num in range(10):\n",
    "        begin = '<item>'\n",
    "        trends_title = (\n",
    "            '<div style=\"text-align:left;font-size:20px;\">'+\n",
    "            '<span style=\"margin-left:20px;font-family:Lucida Console;color:#008000;\">({})</span>'.format(num+1)+\n",
    "            '<span style=\"margin-left:50px;margin-bottom:5px;font-weight:bold;\">{}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span>'.format(trends_title_list[num])+\n",
    "            '<span>🔥&nbsp;{}筆搜尋</span></div>'.format(trends_search_list[num])\n",
    "            )\n",
    "        news = '<div style=\"text-align:left;margin-bottom:25px;margin-left:50px;font-size:20px;\"><a href=\"{}\" target=\"_blank\">{}</a></div>'.format(trends_url_list[num],trends_articles_list[num])\n",
    "        end = '</item>'\n",
    "        rw.write(begin+trends_title+news+end)\n",
    "        \n",
    "except Exception as errormsg:\n",
    "    print('google資料獲取失敗')\n",
    "    today = datetime.now().strftime('%Y-%m-%d %H時')\n",
    "    message = 'google資料獲取有誤，發生時間點：{}\\n'.format(today)\n",
    "    lineNotifyMessage(token, message+str(errormsg))\n",
    "    time.sleep(5)\n",
    "    os._exit(0)\n",
    "rw.close()\n",
    "\n",
    "\n",
    "'''1.把 Google 資料拆分後 放入字典'''\n",
    "import jieba\n",
    "import jieba.posseg as pseg\n",
    "\n",
    "\n",
    "try:\n",
    "    for i in range(len(trends_title_list)): \n",
    "        text = trends_title_list[i]\n",
    "        words = pseg.cut(text)\n",
    "        print(f'text:{text}')\n",
    "        for word, flag in words:\n",
    "            #基本字典\n",
    "            if len(word) > 1:\n",
    "                with open('NLP/NLP-CN_words.txt', 'a',encoding='utf-8') as f:\n",
    "                    f.write(word+\"\\n\")\n",
    "\n",
    "            #個人字典\n",
    "            if flag == 'nr' or flag == 'nrfg':\n",
    "                if len(word) > 2:\n",
    "                    print(\"人名：\", word)\n",
    "                    with open('NLP/NLP-People.txt', 'a',encoding='utf-8') as f:\n",
    "                        f.write(word+\"\\n\")\n",
    "\n",
    "            elif flag == 'ns' or flag == 'nrt':\n",
    "                if len(word) >1:\n",
    "                    print(\"地名：\", word)\n",
    "                    with open('NLP/NLP-Region.txt', 'a',encoding='utf-8') as f:\n",
    "                        f.write(word+\"\\n\")\n",
    "\n",
    "            elif flag == 'nt':\n",
    "                if len(word) >1:\n",
    "                    print(\"組織：\", word)\n",
    "                    with open('NLP/NLP-Org.txt', 'a',encoding='utf-8') as f:\n",
    "                        f.write(word+\"\\n\")\n",
    "            else:\n",
    "                if len(word) >1:\n",
    "                    print(\"事件：\", word)\n",
    "                    with open('NLP/NLP-Event.txt', 'a',encoding='utf-8') as f:\n",
    "                        f.write(word+\"\\n\")\n",
    "    f.close()\n",
    "\n",
    "\n",
    "    ### 剔除已有單字\n",
    "    stopList = [\"NLP/NLP_People.txt\",\"NLP/NLP_Region.txt\",\"NLP_Org\"]\n",
    "    stop = [line.strip() for line in open(\"NLP/NLP-stopwords.txt\",encoding=\"utf-8\").readlines()]\n",
    "    with open('NLP/NLP-Event.txt', 'a',encoding='utf-8') as f:\n",
    "                        f.write(word+\"\\n\")\n",
    "\n",
    "\n",
    "    ''' 重複內容過濾'''\n",
    "    StopList = ['NLP/NLP-Region.txt','NLP/NLP-Org.txt','NLP/NLP-People.txt']\n",
    "\n",
    "    with open('NLP/NLP-Event.txt', 'r', encoding='utf-8') as f1:\n",
    "        nlp_event = set(f1.read().splitlines())\n",
    "\n",
    "\n",
    "    for i in range(len(StopList)):\n",
    "        with open(StopList[i], 'r', encoding='utf-8') as f2:\n",
    "            nlp_stop = set(f2.read().splitlines())\n",
    "\n",
    "\n",
    "        overlap = nlp_event.intersection(nlp_stop)\n",
    "        if overlap:\n",
    "            print(\"有重複的內容：\", overlap)\n",
    "            with open('NLP/NLP-Event.txt', 'r+', encoding='utf-8') as f5:\n",
    "                lines = f5.readlines()\n",
    "                f5.seek(0)\n",
    "                for line in lines:\n",
    "                    if line.strip() not in overlap:\n",
    "                        f5.write(line)\n",
    "                f5.truncate()\n",
    "            print(\"重複的內容已自動刪除\")\n",
    "        else:\n",
    "            print(\"沒有重複的內容\")\n",
    "\n",
    "    ### 刪除重複字元\n",
    "    def overwrite(txt):  \n",
    "        with open(txt, 'r',encoding=\"utf-8\") as f:\n",
    "            lines = f.readlines()\n",
    "        lines = list(set(lines))\n",
    "        with open(txt, 'w',encoding=\"utf-8\") as f:\n",
    "            f.writelines(lines)\n",
    "\n",
    "    overwrite('NLP/NLP-People.txt')\n",
    "    overwrite('NLP/NLP-Region.txt')\n",
    "    overwrite('NLP/NLP-Event.txt')\n",
    "    overwrite('NLP/NLP-Org.txt')\n",
    "    overwrite('NLP/NLP-CN_words.txt')\n",
    "    overwrite('NLP/NLP-stopwords.txt')    \n",
    "    \n",
    "except Exception as errormsg:\n",
    "    print('寫入關鍵字失敗')\n",
    "    time.sleep(5)\n",
    "    os._exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd5842c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trends_title_list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
