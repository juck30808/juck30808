{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d23b6c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "path = 'test.txt'\n",
    "f = open(path, 'w')\n",
    "f.write('Hello World')\n",
    "f.write(\"123\")\n",
    "f.write(\"123.45\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ee8ffff",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'test.txt'\n",
    "f = open(path, 'w')\n",
    "lines = ['Hello World\\n', '123', '456\\n', '789\\n']\n",
    "f.writelines(lines)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbcb6afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'test.txt'\n",
    "f = open(path, 'a')\n",
    "lines = ['Hello World\\n', '123', '456\\n', '789\\n']\n",
    "f.writelines(lines)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d70eb4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'test.txt'\n",
    "f = open(path, 'w')\n",
    "print('Hello World', file=f)\n",
    "print('123', file=f)\n",
    "print('456', file=f)\n",
    "print('789', file=f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2ff1dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'test.txt'\n",
    "with open(path, 'w') as f:\n",
    "    f.write('apple\\n')\n",
    "    f.write('banana\\n')\n",
    "    f.write('lemon\\n')\n",
    "    f.write('tomato\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "debc9aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'test.txt'\n",
    "with open(path, 'a') as f:\n",
    "    f.write('apple\\n')\n",
    "    f.write('banana\\n')\n",
    "    f.write('tomato\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74ee1879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "google access ok\n",
      "ç²å–æ¨™é¡Œç­†æ•¸å…±ï¼š 31 ç­†\n",
      "ç²å–æœå°‹é‡ç­†æ•¸å…±ï¼š 31 ç­†\n",
      "ç²å–å…§æ–‡ç­†æ•¸å…±ï¼š 31 ç­†\n",
      "ç²å–è¶…é€£çµç­†æ•¸å…±ï¼š 31 ç­†\n",
      "text:å³éœ\n",
      "text:è¬äº¬ç©\n",
      "äº‹ä»¶ï¼š è¬äº¬ç©\n",
      "text:æ›¹èˆˆèª \n",
      "äº‹ä»¶ï¼š æ›¹èˆˆèª \n",
      "text:æ¥Šä¸ç³\n",
      "äº‹ä»¶ï¼š æ¥Šä¸ç³\n",
      "text:æ›¼åŸ\n",
      "åœ°åï¼š æ›¼åŸ\n",
      "text:è–©çˆ¾é”\n",
      "äº‹ä»¶ï¼š è–©çˆ¾é”\n",
      "text:å·å£æ˜¥å¥ˆ\n",
      "åœ°åï¼š å·å£\n",
      "äº‹ä»¶ï¼š æ˜¥å¥ˆ\n",
      "text:è¬å®¢ä»€é‹\n",
      "äº‹ä»¶ï¼š è¬å®¢ä»€é‹\n",
      "text:POCO F5\n",
      "äº‹ä»¶ï¼š POCO\n",
      "äº‹ä»¶ï¼š F5\n",
      "text:TEEN TOP\n",
      "äº‹ä»¶ï¼š TEEN\n",
      "äº‹ä»¶ï¼š TOP\n",
      "text:å…ƒæ™¶\n",
      "äº‹ä»¶ï¼š å…ƒæ™¶\n",
      "text:å‹‡å£«\n",
      "äº‹ä»¶ï¼š å‹‡å£«\n",
      "text:Coldplay\n",
      "äº‹ä»¶ï¼š Coldplay\n",
      "text:æå¤§ç¶­\n",
      "äº‹ä»¶ï¼š æå¤§ç¶­\n",
      "text:é™³æ•è–°\n",
      "äººåï¼š é™³æ•è–°\n",
      "text:è—å£½å¸æ——è‰¦åº—\n",
      "äº‹ä»¶ï¼š å£½å¸\n",
      "äº‹ä»¶ï¼š æ——è‰¦\n",
      "text:æ¥Šå®¶ä¿\n",
      "äº‹ä»¶ï¼š æ¥Šå®¶ä¿\n",
      "text:é•·æœˆç‡¼æ˜çµå±€\n",
      "äº‹ä»¶ï¼š æœˆç‡¼\n",
      "äº‹ä»¶ï¼š çµå±€\n",
      "text:é­”æ³•é˜¿å¬¤\n",
      "äº‹ä»¶ï¼š é­”æ³•\n",
      "äº‹ä»¶ï¼š é˜¿å¬¤\n",
      "text:å·¨é½’é¯Š2\n",
      "äº‹ä»¶ï¼š å·¨é½’\n",
      "text:æŒ–å‘€æŒ–è€å¸«\n",
      "äº‹ä»¶ï¼š è€å¸«\n",
      "text:é™¶å–†\n",
      "äº‹ä»¶ï¼š é™¶å–†\n",
      "text:å·¨é½’é¯Š\n",
      "äº‹ä»¶ï¼š å·¨é½’\n",
      "text:å †é«˜æ©Ÿ\n",
      "äº‹ä»¶ï¼š å †é«˜æ©Ÿ\n",
      "text:ç†±ç«\n",
      "äº‹ä»¶ï¼š ç†±ç«\n",
      "text:é„­å®¹å’Œ\n",
      "äº‹ä»¶ï¼š é„­å®¹\n",
      "text:CPI\n",
      "äº‹ä»¶ï¼š CPI\n",
      "text:èŒ²å¡ç—…æ¯’\n",
      "åœ°åï¼š èŒ²å¡\n",
      "äº‹ä»¶ï¼š ç—…æ¯’\n",
      "text:Sum41\n",
      "äº‹ä»¶ï¼š Sum\n",
      "äº‹ä»¶ï¼š 41\n",
      "text:è”¡å¤©é³³\n",
      "äº‹ä»¶ï¼š è”¡å¤©é³³\n",
      "text:å…‰èˆ‡å¤œä¹‹æˆ€\n",
      "äº‹ä»¶ï¼š å¤œä¹‹æˆ€\n",
      "æ²’æœ‰é‡è¤‡çš„å…§å®¹\n",
      "æœ‰é‡è¤‡çš„å…§å®¹ï¼š {'ç¾åœ‹äºº'}\n",
      "é‡è¤‡çš„å…§å®¹å·²è‡ªå‹•åˆªé™¤\n",
      "æœ‰é‡è¤‡çš„å…§å®¹ï¼š {'è¬äº¬ç©', 'è”¡å¤©é³³', 'æ¥Šå®¶ä¿', 'æ›¹èˆˆèª ', 'æ¥Šä¸ç³', 'æå¤§ç¶­', 'è–©çˆ¾é”'}\n",
      "é‡è¤‡çš„å…§å®¹å·²è‡ªå‹•åˆªé™¤\n"
     ]
    }
   ],
   "source": [
    "import requests,json,re,os,time\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "from jsonpath import jsonpath\n",
    "from fake_useragent import UserAgent\n",
    "#pip install jieba\n",
    "#pip install BeautifulSoup4\n",
    "#pip install jsonpath\n",
    "#pip install fake-useragent\n",
    "\n",
    "# Google Trend\n",
    "# æ•™å­¸ç¶²å€ https://tlyu0419.github.io/2020/02/18/Crawl-GoogleTrends/\n",
    "\n",
    "\n",
    "# è¨­å®š line æ¨æ’­å‡½å¼\n",
    "# def lineNotifyMessage(token, msg):\n",
    "#     headers = {\n",
    "#         \"Authorization\": \"Bearer \" + token, \n",
    "#         \"Content-Type\" : \"application/x-www-form-urlencoded\"\n",
    "#     }\n",
    "\n",
    "#     payload = {'message': msg }\n",
    "#     r = requests.post(\"https://notify-api.line.me/api/notify\", headers = headers, params = payload)\n",
    "#     return r.status_code\n",
    "# token = 'm6uafWsyIziRWXaqYfTKJxNShGYlp3WM3RG9e0hP2OA'\n",
    "\n",
    "\n",
    "time_trend = datetime.now().strftime('%Y%m%d')\n",
    "time_trend = str(time_trend)\n",
    "\n",
    "# å˜—è©¦ç·¨ç¢¼æ“·å–è³‡æ–™ï¼Œè‹¥å‡ºç¾éŒ¯èª¤å°±ä¸­æ–·åŸ·è¡Œ\n",
    "try:\n",
    "    # ä¿®æ”¹ç¶²å€ç‚ºç›®å‰æ™‚é–“\n",
    "    trends_url = 'https://trends.google.com.tw/trends/api/dailytrends?hl=zh-TW&tz=-480&ed={}&geo=TW&ns=15'.format(time_trend)\n",
    "\n",
    "    res_trends = requests.get(trends_url)\n",
    "\n",
    "    # æŠŠå¹²æ“¾æ–‡å­— )]}',\\n åˆªé™¤ \n",
    "    res_trends = re.sub(r'\\)\\]\\}\\',\\n', '', res_trends.text)\n",
    "\n",
    "    # è½‰ç¢¼ï¼Œä¸ç„¶ä¸èƒ½çœ‹\n",
    "\n",
    "    trends = res_trends.encode('utf-8').decode('unicode_escape')\n",
    "\n",
    "\n",
    "    # æŠŠjsonæª”å†è½‰æˆå­—ä¸²é–‹å§‹åšæ­£å‰‡è™•ç†\n",
    "    trends_str = str(trends)\n",
    "    trends_str = trends_str.replace('\\\"','\\'')\n",
    "\n",
    "    trends_content = re.findall(\"'query'.*?'snippet':\",trends_str)\n",
    "\n",
    "    trends_title_list = []\n",
    "    trends_search_list = []\n",
    "    trends_articles_list = []\n",
    "    trends_url_list = []\n",
    "    count_trends = 0\n",
    "\n",
    "    for content in trends_content:\n",
    "        count_trends+=1\n",
    "        # æ´—å‡ºæ¨™é¡Œ\n",
    "        trends_title = re.findall(\"'query'.*?,\",content)\n",
    "        trends_title = trends_title[0]\n",
    "        trends_title = trends_title[9:-2]\n",
    "        trends_title_list.append(trends_title)\n",
    "\n",
    "        # æ´—å‡ºæœå°‹é‡\n",
    "        trends_search = re.findall(\"formattedTraffic.*?,\",content)\n",
    "        trends_search = trends_search[0]\n",
    "        trends_search = trends_search[19:-2]\n",
    "        trends_search_list.append(trends_search)    \n",
    "\n",
    "        # æ´—å‡ºç¬¬ä¸€å‰‡æ–‡ç« \n",
    "        trends_articles = re.findall(\"'articles.*?timeAgo\",content)\n",
    "        trends_articles = trends_articles[0]\n",
    "        trends_articles = trends_articles[22:-10]\n",
    "        trends_articles_list.append(trends_articles)\n",
    "        \n",
    "        # æ´—å‡ºæ–‡ç« è¶…é€£çµï¼ŒæŠ“ä¸åˆ°å°±ç”¨googleæœå°‹æ›¿ä»£\n",
    "        if 'newsUrl' or '\\'Url' in content:\n",
    "            trends_url = re.findall(\"newsUrl.*?',|'url':.*?',\",content)\n",
    "            trends_url = trends_url[0]\n",
    "            trends_url = re.findall(\"http.*?',\",content)\n",
    "            trends_url = trends_url[0]\n",
    "            trends_url = trends_url[:-2]\n",
    "            trends_url_list.append(trends_url)\n",
    "        else:\n",
    "            trends_url = 'https://www.google.com/search?q='+trends_title\n",
    "            trends_url_list.append(trends_url)\n",
    "\n",
    "\n",
    "    print('google access ok')\n",
    "    print('ç²å–æ¨™é¡Œç­†æ•¸å…±ï¼š {} ç­†'.format(len(trends_title_list)))\n",
    "    print('ç²å–æœå°‹é‡ç­†æ•¸å…±ï¼š {} ç­†'.format(len(trends_search_list)))\n",
    "    print('ç²å–å…§æ–‡ç­†æ•¸å…±ï¼š {} ç­†'.format(len(trends_articles_list)))\n",
    "    print('ç²å–è¶…é€£çµç­†æ•¸å…±ï¼š {} ç­†'.format(len(trends_url_list)))\n",
    "\n",
    "\n",
    "    ################ ç·¨å¯«ç¶²é å…§å®¹  ################\n",
    "\n",
    "    rw = open('separate/google.html','w',encoding = 'utf8')\n",
    "    # r è®€å–\n",
    "    # w å¯«å…¥(åˆªé™¤åŸæœ¬å…§å®¹)\n",
    "    # a è¿½åŠ å¯«å…¥\n",
    "\n",
    "\n",
    "    # å¯«å…¥google trends\n",
    "    title_trends = '<h2 style=\"text-align:center;background: #f00; color: #fff; margin: 0 4px; border-radius: 4px;\">Googleæœå°‹è¶¨å‹¢</h2>'\n",
    "    rw.write(title_trends)\n",
    "\n",
    "    # å¯«å…¥è³‡æ–™æ“·å–æ™‚é–“\n",
    "    time_now = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    time_news = '<div><h3 style=\"text-align:center;color:gray\">è³‡æ–™æ“·å–æ™‚é–“{}</h2></div>'.format(time_now)\n",
    "    rw.write(time_news)\n",
    "\n",
    "    for num in range(10):\n",
    "        begin = '<item>'\n",
    "        trends_title = (\n",
    "            '<div style=\"text-align:left;font-size:20px;\">'+\n",
    "            '<span style=\"margin-left:20px;font-family:Lucida Console;color:#008000;\">({})</span>'.format(num+1)+\n",
    "            '<span style=\"margin-left:50px;margin-bottom:5px;font-weight:bold;\">{}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span>'.format(trends_title_list[num])+\n",
    "            '<span>ğŸ”¥&nbsp;{}ç­†æœå°‹</span></div>'.format(trends_search_list[num])\n",
    "            )\n",
    "        news = '<div style=\"text-align:left;margin-bottom:25px;margin-left:50px;font-size:20px;\"><a href=\"{}\" target=\"_blank\">{}</a></div>'.format(trends_url_list[num],trends_articles_list[num])\n",
    "        end = '</item>'\n",
    "        rw.write(begin+trends_title+news+end)\n",
    "        \n",
    "except Exception as errormsg:\n",
    "    print('googleè³‡æ–™ç²å–å¤±æ•—')\n",
    "    today = datetime.now().strftime('%Y-%m-%d %Hæ™‚')\n",
    "    message = 'googleè³‡æ–™ç²å–æœ‰èª¤ï¼Œç™¼ç”Ÿæ™‚é–“é»ï¼š{}\\n'.format(today)\n",
    "    lineNotifyMessage(token, message+str(errormsg))\n",
    "    time.sleep(5)\n",
    "    os._exit(0)\n",
    "rw.close()\n",
    "\n",
    "\n",
    "'''1.æŠŠ Google è³‡æ–™æ‹†åˆ†å¾Œ æ”¾å…¥å­—å…¸'''\n",
    "import jieba\n",
    "import jieba.posseg as pseg\n",
    "\n",
    "\n",
    "try:\n",
    "    for i in range(len(trends_title_list)): \n",
    "        text = trends_title_list[i]\n",
    "        words = pseg.cut(text)\n",
    "        print(f'text:{text}')\n",
    "        for word, flag in words:\n",
    "            #åŸºæœ¬å­—å…¸\n",
    "            if len(word) > 1:\n",
    "                with open('NLP/NLP-CN_words.txt', 'a',encoding='utf-8') as f:\n",
    "                    f.write(word+\"\\n\")\n",
    "\n",
    "            #å€‹äººå­—å…¸\n",
    "            if flag == 'nr' or flag == 'nrfg':\n",
    "                if len(word) > 2:\n",
    "                    print(\"äººåï¼š\", word)\n",
    "                    with open('NLP/NLP-People.txt', 'a',encoding='utf-8') as f:\n",
    "                        f.write(word+\"\\n\")\n",
    "\n",
    "            elif flag == 'ns' or flag == 'nrt':\n",
    "                if len(word) >1:\n",
    "                    print(\"åœ°åï¼š\", word)\n",
    "                    with open('NLP/NLP-Region.txt', 'a',encoding='utf-8') as f:\n",
    "                        f.write(word+\"\\n\")\n",
    "\n",
    "            elif flag == 'nt':\n",
    "                if len(word) >1:\n",
    "                    print(\"çµ„ç¹”ï¼š\", word)\n",
    "                    with open('NLP/NLP-Org.txt', 'a',encoding='utf-8') as f:\n",
    "                        f.write(word+\"\\n\")\n",
    "            else:\n",
    "                if len(word) >1:\n",
    "                    print(\"äº‹ä»¶ï¼š\", word)\n",
    "                    with open('NLP/NLP-Event.txt', 'a',encoding='utf-8') as f:\n",
    "                        f.write(word+\"\\n\")\n",
    "    f.close()\n",
    "\n",
    "\n",
    "    ### å‰”é™¤å·²æœ‰å–®å­—\n",
    "    stopList = [\"NLP/NLP_People.txt\",\"NLP/NLP_Region.txt\",\"NLP_Org\"]\n",
    "    stop = [line.strip() for line in open(\"NLP/NLP-stopwords.txt\",encoding=\"utf-8\").readlines()]\n",
    "    with open('NLP/NLP-Event.txt', 'a',encoding='utf-8') as f:\n",
    "                        f.write(word+\"\\n\")\n",
    "\n",
    "\n",
    "    ''' é‡è¤‡å…§å®¹éæ¿¾'''\n",
    "    StopList = ['NLP/NLP-Region.txt','NLP/NLP-Org.txt','NLP/NLP-People.txt']\n",
    "\n",
    "    with open('NLP/NLP-Event.txt', 'r', encoding='utf-8') as f1:\n",
    "        nlp_event = set(f1.read().splitlines())\n",
    "\n",
    "\n",
    "    for i in range(len(StopList)):\n",
    "        with open(StopList[i], 'r', encoding='utf-8') as f2:\n",
    "            nlp_stop = set(f2.read().splitlines())\n",
    "\n",
    "\n",
    "        overlap = nlp_event.intersection(nlp_stop)\n",
    "        if overlap:\n",
    "            print(\"æœ‰é‡è¤‡çš„å…§å®¹ï¼š\", overlap)\n",
    "            with open('NLP/NLP-Event.txt', 'r+', encoding='utf-8') as f5:\n",
    "                lines = f5.readlines()\n",
    "                f5.seek(0)\n",
    "                for line in lines:\n",
    "                    if line.strip() not in overlap:\n",
    "                        f5.write(line)\n",
    "                f5.truncate()\n",
    "            print(\"é‡è¤‡çš„å…§å®¹å·²è‡ªå‹•åˆªé™¤\")\n",
    "        else:\n",
    "            print(\"æ²’æœ‰é‡è¤‡çš„å…§å®¹\")\n",
    "\n",
    "    ### åˆªé™¤é‡è¤‡å­—å…ƒ\n",
    "    def overwrite(txt):  \n",
    "        with open(txt, 'r',encoding=\"utf-8\") as f:\n",
    "            lines = f.readlines()\n",
    "        lines = list(set(lines))\n",
    "        with open(txt, 'w',encoding=\"utf-8\") as f:\n",
    "            f.writelines(lines)\n",
    "\n",
    "    overwrite('NLP/NLP-People.txt')\n",
    "    overwrite('NLP/NLP-Region.txt')\n",
    "    overwrite('NLP/NLP-Event.txt')\n",
    "    overwrite('NLP/NLP-Org.txt')\n",
    "    overwrite('NLP/NLP-CN_words.txt')\n",
    "    overwrite('NLP/NLP-stopwords.txt')    \n",
    "    \n",
    "except Exception as errormsg:\n",
    "    print('å¯«å…¥é—œéµå­—å¤±æ•—')\n",
    "    time.sleep(5)\n",
    "    os._exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd5842c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trends_title_list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
